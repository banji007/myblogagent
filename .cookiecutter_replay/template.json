{
  "cookiecutter": {
    "project_name": "myblogagent",
    "agent_name": "remote_2022783209315744818",
    "package_version": "0.14.1",
    "agent_description": "AI-driven agent designed to facilitate the exploration of the blogger agent landscape",
    "example_question": "Write an article about Nano Banana                           ",
    "settings": {
      "requires_data_ingestion": false,
      "requires_session": true,
      "deployment_targets": [
        "agent_engine",
        "cloud_run"
      ],
      "extra_dependencies": [
        "google-adk~=1.8.0"
      ],
      "tags": [
        "adk"
      ],
      "frontend_type": "None",
      "agent_directory": "blogger_agent"
    },
    "tags": "adk",
    "deployment_target": "cloud_run",
    "cicd_runner": "github_actions",
    "session_type": "in_memory",
    "frontend_type": "None",
    "extra_dependencies": [
      "google-adk~=1.8.0"
    ],
    "data_ingestion": false,
    "datastore_type": "",
    "agent_directory": "blogger_agent",
    "agent_garden": false,
    "adk_cheatsheet": "# Google Agent Development Kit (ADK) Python Cheatsheet\n\nThis document serves as a long-form, comprehensive reference for building, orchestrating, and deploying AI agents using the Python Agent Development Kit (ADK). It aims to cover every significant aspect with greater detail, more code examples, and in-depth best practices.\n\n## Table of Contents\n\n1.  [Core Concepts & Project Structure](#1-core-concepts--project-structure)\n    *   1.1 ADK's Foundational Principles\n    *   1.2 Essential Primitives\n    *   1.3 Standard Project Layout\n    *   1.A Build Agents without Code (Agent Config)\n2.  [Agent Definitions (`LlmAgent`)](#2-agent-definitions-llmagent)\n    *   2.1 Basic `LlmAgent` Setup\n    *   2.2 Advanced `LlmAgent` Configuration\n    *   2.3 LLM Instruction Crafting\n3.  [Orchestration with Workflow Agents](#3-orchestration-with-workflow-agents)\n    *   3.1 `SequentialAgent`: Linear Execution\n    *   3.2 `ParallelAgent`: Concurrent Execution\n    *   3.3 `LoopAgent`: Iterative Processes\n4.  [Multi-Agent Systems & Communication](#4-multi-agent-systems--communication)\n    *   4.1 Agent Hierarchy\n    *   4.2 Inter-Agent Communication Mechanisms\n    *   4.3 Common Multi-Agent Patterns\n    *   4.A Distributed Communication (A2A Protocol)\n5.  [Building Custom Agents (`BaseAgent`)](#5-building-custom-agents-baseagent)\n    *   5.1 When to Use Custom Agents\n    *   5.2 Implementing `_run_async_impl`\n6.  [Models: Gemini, LiteLLM, and Vertex AI](#6-models-gemini-litellm-and-vertex-ai)\n    *   6.1 Google Gemini Models (AI Studio & Vertex AI)\n    *   6.2 Other Cloud & Proprietary Models via LiteLLM\n    *   6.3 Open & Local Models via LiteLLM (Ollama, vLLM)\n    *   6.4 Customizing LLM API Clients\n7.  [Tools: The Agent's Capabilities](#7-tools-the-agents-capabilities)\n    *   7.1 Defining Function Tools: Principles & Best Practices\n    *   7.2 The `ToolContext` Object: Accessing Runtime Information\n    *   7.3 All Tool Types & Their Usage\n8.  [Context, State, and Memory Management](#8-context-state-and-memory-management)\n    *   8.1 The `Session` Object & `SessionService`\n    *   8.2 `State`: The Conversational Scratchpad\n    *   8.3 `Memory`: Long-Term Knowledge & Retrieval\n    *   8.4 `Artifacts`: Binary Data Management\n9.  [Runtime, Events, and Execution Flow](#9-runtime-events-and-execution-flow)\n    *   9.1 The `Runner`: The Orchestrator\n    *   9.2 The Event Loop: Core Execution Flow\n    *   9.3 `Event` Object: The Communication Backbone\n    *   9.4 Asynchronous Programming (Python Specific)\n10. [Control Flow with Callbacks](#10-control-flow-with-callbacks)\n    *   10.1 Callback Mechanism: Interception & Control\n    *   10.2 Types of Callbacks\n    *   10.3 Callback Best Practices\n    *   10.A Global Control with Plugins\n11. [Authentication for Tools](#11-authentication-for-tools)\n    *   11.1 Core Concepts: `AuthScheme` & `AuthCredential`\n    *   11.2 Interactive OAuth/OIDC Flows\n    *   11.3 Custom Tool Authentication\n12. [Deployment Strategies](#12-deployment-strategies)\n    *   12.1 Local Development & Testing (`adk web`, `adk run`, `adk api_server`)\n    *   12.2 Vertex AI Agent Engine\n    *   12.3 Cloud Run\n    *   12.4 Google Kubernetes Engine (GKE)\n    *   12.5 CI/CD Integration\n13. [Evaluation and Safety](#13-evaluation-and-safety)\n    *   13.1 Agent Evaluation (`adk eval`)\n    *   13.2 Safety & Guardrails\n14. [Debugging, Logging & Observability](#14-debugging-logging--observability)\n15. [Streaming & Advanced I/O](#15-streaming--advanced-io)\n16. [Performance Optimization](#16-performance-optimization)\n17. [General Best Practices & Common Pitfalls](#17-general-best-practices--common-pitfalls)\n18. [Official API & CLI References](#18-official-api--cli-references)\n\n---\n\n## 1. Core Concepts & Project Structure\n\n### 1.1 ADK's Foundational Principles\n\n*   **Modularity**: Break down complex problems into smaller, manageable agents and tools.\n*   **Composability**: Combine simple agents and tools to build sophisticated systems.\n*   **Observability**: Detailed event logging and tracing capabilities to understand agent behavior.\n*   **Extensibility**: Easily integrate with external services, models, and frameworks.\n*   **Deployment-Agnostic**: Design agents once, deploy anywhere.\n\n### 1.2 Essential Primitives\n\n*   **`Agent`**: The core intelligent unit. Can be `LlmAgent` (LLM-driven) or `BaseAgent` (custom/workflow).\n*   **`Tool`**: Callable function/class providing external capabilities (`FunctionTool`, `OpenAPIToolset`, etc.).\n*   **`Session`**: A unique, stateful conversation thread with history (`events`) and short-term memory (`state`).\n*   **`State`**: Key-value dictionary within a `Session` for transient conversation data.\n*   **`Memory`**: Long-term, searchable knowledge base beyond a single session (`MemoryService`).\n*   **`Artifact`**: Named, versioned binary data (files, images) associated with a session or user.\n*   **`Runner`**: The execution engine; orchestrates agent activity and event flow.\n*   **`Event`**: Atomic unit of communication and history; carries content and side-effect `actions`.\n*   **`InvocationContext`**: The comprehensive root context object holding all runtime information for a single `run_async` call.\n\n### 1.3 Standard Project Layout\n\nA well-structured ADK project is crucial for maintainability and leveraging `adk` CLI tools.\n\n```\nyour_project_root/\n\u251c\u2500\u2500 my_first_agent/             # Each folder is a distinct agent app\n\u2502   \u251c\u2500\u2500 __init__.py             # Makes `my_first_agent` a Python package (`from . import agent`)\n\u2502   \u251c\u2500\u2500 agent.py                # Contains `root_agent` definition and `LlmAgent`/WorkflowAgent instances\n\u2502   \u251c\u2500\u2500 tools.py                # Custom tool function definitions\n\u2502   \u251c\u2500\u2500 data/                   # Optional: static data, templates\n\u2502   \u2514\u2500\u2500 .env                    # Environment variables (API keys, project IDs)\n\u251c\u2500\u2500 my_second_agent/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 agent.py\n\u251c\u2500\u2500 requirements.txt            # Project's Python dependencies (e.g., google-adk, litellm)\n\u251c\u2500\u2500 tests/                      # Unit and integration tests\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2502   \u2514\u2500\u2500 test_tools.py\n\u2502   \u2514\u2500\u2500 integration/\n\u2502       \u2514\u2500\u2500 test_my_first_agent.py\n\u2502       \u2514\u2500\u2500 my_first_agent.evalset.json # Evaluation dataset for `adk eval`\n\u2514\u2500\u2500 main.py                     # Optional: Entry point for custom FastAPI server deployment\n```\n*   `adk web` and `adk run` automatically discover agents in subdirectories with `__init__.py` and `agent.py`.\n*   `.env` files are automatically loaded by `adk` tools when run from the root or agent directory.\n\n### 1.A Build Agents without Code (Agent Config)\n\nADK allows you to define agents, tools, and even multi-agent workflows using a simple YAML format, eliminating the need to write Python code for orchestration. This is ideal for rapid prototyping and for non-programmers to configure agents.\n\n#### **Getting Started with Agent Config**\n\n*   **Create a Config-based Agent**:\n    ```bash\n    adk create --type=config my_yaml_agent\n    ```\n    This generates a `my_yaml_agent/` folder with `root_agent.yaml` and `.env` files.\n\n*   **Environment Setup** (in `.env` file):\n    ```bash\n    # For Google AI Studio (simpler setup)\n    GOOGLE_GENAI_USE_VERTEXAI=0\n    GOOGLE_API_KEY=<your-Google-Gemini-API-key>\n    \n    # For Google Cloud Vertex AI (production)\n    GOOGLE_GENAI_USE_VERTEXAI=1\n    GOOGLE_CLOUD_PROJECT=<your_gcp_project>\n    GOOGLE_CLOUD_LOCATION=us-central1\n    ```\n\n#### **Core Agent Config Structure**\n\n*   **Basic Agent (`root_agent.yaml`)**:\n    ```yaml\n    # yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json\n    name: assistant_agent\n    model: gemini-2.5-flash\n    description: A helper agent that can answer users' various questions.\n    instruction: You are an agent to help answer users' various questions.\n    ```\n\n*   **Agent with Built-in Tools**:\n    ```yaml\n    name: search_agent\n    model: gemini-2.0-flash\n    description: 'an agent whose job it is to perform Google search queries and answer questions about the results.'\n    instruction: You are an agent whose job is to perform Google search queries and answer questions about the results.\n    tools:\n      - name: google_search # Built-in ADK tool\n    ```\n\n*   **Agent with Custom Tools**:\n    ```yaml\n    agent_class: LlmAgent\n    model: gemini-2.5-flash\n    name: prime_agent\n    description: Handles checking if numbers are prime.\n    instruction: |\n      You are responsible for checking whether numbers are prime.\n      When asked to check primes, you must call the check_prime tool with a list of integers.\n      Never attempt to determine prime numbers manually.\n    tools:\n      - name: ma_llm.check_prime # Reference to Python function\n    ```\n\n*   **Multi-Agent System with Sub-Agents**:\n    ```yaml\n    agent_class: LlmAgent\n    model: gemini-2.5-flash\n    name: root_agent\n    description: Learning assistant that provides tutoring in code and math.\n    instruction: |\n      You are a learning assistant that helps students with coding and math questions.\n      \n      You delegate coding questions to the code_tutor_agent and math questions to the math_tutor_agent.\n      \n      Follow these steps:\n      1. If the user asks about programming or coding, delegate to the code_tutor_agent.\n      2. If the user asks about math concepts or problems, delegate to the math_tutor_agent.\n      3. Always provide clear explanations and encourage learning.\n    sub_agents:\n      - config_path: code_tutor_agent.yaml\n      - config_path: math_tutor_agent.yaml\n    ```\n\n#### **Loading Agent Config in Python**\n\n```python\nfrom google.adk.agents import config_agent_utils\nroot_agent = config_agent_utils.from_config(\"{agent_folder}/root_agent.yaml\")\n```\n\n#### **Running Agent Config Agents**\n\nFrom the agent directory, use any of these commands:\n*   `adk web` - Launch web UI interface\n*   `adk run` - Run in terminal without UI\n*   `adk api_server` - Run as a service for other applications\n\n#### **Deployment Support**\n\nAgent Config agents can be deployed using:\n*   `adk deploy cloud_run` - Deploy to Google Cloud Run\n*   `adk deploy agent_engine` - Deploy to Vertex AI Agent Engine\n\n#### **Key Features & Capabilities**\n\n*   **Supported Built-in Tools**: `google_search`, `load_artifacts`, `url_context`, `exit_loop`, `preload_memory`, `get_user_choice`, `enterprise_web_search`, `load_web_page`\n*   **Custom Tool Integration**: Reference Python functions using fully qualified module paths\n*   **Multi-Agent Orchestration**: Link agents via `config_path` references\n*   **Schema Validation**: Built-in YAML schema for IDE support and validation\n\n#### **Current Limitations** (Experimental Feature)\n\n*   **Model Support**: Only Gemini models currently supported\n*   **Language Support**: Custom tools must be written in Python\n*   **Unsupported Agent Types**: `LangGraphAgent`, `A2aAgent`\n*   **Unsupported Tools**: `AgentTool`, `LongRunningFunctionTool`, `VertexAiSearchTool`, `MCPToolset`, `CrewaiTool`, `LangchainTool`, `ExampleTool`\n\nFor complete examples and reference, see the [ADK samples repository](https://github.com/search?q=repo%3Agoogle%2Fadk-python+path%3A%2F%5Econtributing%5C%2Fsamples%5C%2F%2F+.yaml&type=code).\n\n---\n\n## 2. Agent Definitions (`LlmAgent`)\n\nThe `LlmAgent` is the cornerstone of intelligent behavior, leveraging an LLM for reasoning and decision-making.\n\n### 2.1 Basic `LlmAgent` Setup\n\n```python\nfrom google.adk.agents import Agent\n\ndef get_current_time(city: str) -> dict:\n    \"\"\"Returns the current time in a specified city.\"\"\"\n    # Mock implementation\n    if city.lower() == \"new york\":\n        return {\"status\": \"success\", \"time\": \"10:30 AM EST\"}\n    return {\"status\": \"error\", \"message\": f\"Time for {city} not available.\"}\n\nmy_first_llm_agent = Agent(\n    name=\"time_teller_agent\",\n    model=\"gemini-2.5-flash\", # Essential: The LLM powering the agent\n    instruction=\"You are a helpful assistant that tells the current time in cities. Use the 'get_current_time' tool for this purpose.\",\n    description=\"Tells the current time in a specified city.\", # Crucial for multi-agent delegation\n    tools=[get_current_time] # List of callable functions/tool instances\n)\n```\n\n### 2.2 Advanced `LlmAgent` Configuration\n\n*   **`generate_content_config`**: Controls LLM generation parameters (temperature, token limits, safety).\n    ```python\n    from google.genai import types as genai_types\n    from google.adk.agents import Agent\n\n    gen_config = genai_types.GenerateContentConfig(\n        temperature=0.2,            # Controls randomness (0.0-1.0), lower for more deterministic.\n        top_p=0.9,                  # Nucleus sampling: sample from top_p probability mass.\n        top_k=40,                   # Top-k sampling: sample from top_k most likely tokens.\n        max_output_tokens=1024,     # Max tokens in LLM's response.\n        stop_sequences=[\"## END\"]   # LLM will stop generating if these sequences appear.\n    )\n    agent = Agent(\n        # ... basic config ...\n        generate_content_config=gen_config\n    )\n    ```\n\n*   **`output_key`**: Automatically saves the agent's final text or structured (if `output_schema` is used) response to the `session.state` under this key. Facilitates data flow between agents.\n    ```python\n    agent = Agent(\n        # ... basic config ...\n        output_key=\"llm_final_response_text\"\n    )\n    # After agent runs, session.state['llm_final_response_text'] will contain its output.\n    ```\n\n*   **`input_schema` & `output_schema`**: Define strict JSON input/output formats using Pydantic models.\n    > **Warning**: Using `output_schema` forces the LLM to generate JSON and **disables** its ability to use tools or delegate to other agents.\n\n#### **Example: Defining and Using Structured Output**\n\nThis is the most reliable way to make an LLM produce predictable, parseable JSON, which is essential for multi-agent workflows.\n\n1.  **Define the Schema with Pydantic:**\n    ```python\n    from pydantic import BaseModel, Field\n    from typing import Literal\n\n    class SearchQuery(BaseModel):\n        \"\"\"Model representing a specific search query for web search.\"\"\"\n        search_query: str = Field(\n            description=\"A highly specific and targeted query for web search.\"\n        )\n\n    class Feedback(BaseModel):\n        \"\"\"Model for providing evaluation feedback on research quality.\"\"\"\n        grade: Literal[\"pass\", \"fail\"] = Field(\n            description=\"Evaluation result. 'pass' if the research is sufficient, 'fail' if it needs revision.\"\n        )\n        comment: str = Field(\n            description=\"Detailed explanation of the evaluation, highlighting strengths and/or weaknesses of the research.\"\n        )\n        follow_up_queries: list[SearchQuery] | None = Field(\n            default=None,\n            description=\"A list of specific, targeted follow-up search queries needed to fix research gaps. This should be null or empty if the grade is 'pass'.\"\n        )\n    ```\n    *   **`BaseModel` & `Field`**: Define data types, defaults, and crucial `description` fields. These descriptions are sent to the LLM to guide its output.\n    *   **`Literal`**: Enforces strict enum-like values (`\"pass\"` or `\"fail\"`), preventing the LLM from hallucinating unexpected values.\n\n2.  **Assign the Schema to an `LlmAgent`:**\n    ```python\n    research_evaluator = LlmAgent(\n        name=\"research_evaluator\",\n        model=\"gemini-2.5-pro\",\n        instruction=\"\"\"You are a meticulous quality assurance analyst. Evaluate the research findings in 'section_research_findings' and be very critical.\n        If you find significant gaps, assign a grade of 'fail', write a detailed comment, and generate 5-7 specific follow-up queries.\n        If the research is thorough, grade it 'pass'.\n        Your response must be a single, raw JSON object validating against the 'Feedback' schema.\n        \"\"\",\n        output_schema=Feedback, # This forces the LLM to output JSON matching the Feedback model.\n        output_key=\"research_evaluation\", # The resulting JSON object will be saved to state.\n        disallow_transfer_to_peers=True, # Prevents this agent from delegating. Its job is only to evaluate.\n    )\n    ```\n\n*   **`include_contents`**: Controls whether the conversation history is sent to the LLM.\n    *   `'default'` (default): Sends relevant history.\n    *   `'none'`: Sends no history; agent operates purely on current turn's input and `instruction`. Useful for stateless API wrapper agents.\n    ```python\n    agent = Agent(..., include_contents='none')\n    ```\n\n*   **`planner`**: Assign a `BasePlanner` instance to enable multi-step reasoning.\n    *   **`BuiltInPlanner`**: Leverages a model's native \"thinking\" or planning capabilities (e.g., Gemini).\n        ```python\n        from google.adk.planners import BuiltInPlanner\n        from google.genai.types import ThinkingConfig\n\n        agent = Agent(\n            model=\"gemini-2.5-flash\",\n            planner=BuiltInPlanner(\n                thinking_config=ThinkingConfig(include_thoughts=True)\n            ),\n            # ... tools ...\n        )\n        ```\n    *   **`PlanReActPlanner`**: Instructs the model to follow a structured Plan-Reason-Act output format, useful for models without built-in planning.\n\n*   **`code_executor`**: Assign a `BaseCodeExecutor` to allow the agent to execute code blocks.\n    *   **`BuiltInCodeExecutor`**: The standard, sandboxed code executor provided by ADK for safe execution.\n        ```python\n        from google.adk.code_executors import BuiltInCodeExecutor\n        agent = Agent(\n            name=\"code_agent\",\n            model=\"gemini-2.5-flash\",\n            instruction=\"Write and execute Python code to solve math problems.\",\n            code_executor=BuiltInCodeExecutor() # Corrected from a list to an instance\n        )\n        ```\n\n*   **Callbacks**: Hooks for observing and modifying agent behavior at key lifecycle points (`before_model_callback`, `after_tool_callback`, etc.). (Covered in Callbacks).\n\n### 2.3 LLM Instruction Crafting (`instruction`)\n\nThe `instruction` is critical. It guides the LLM's behavior, persona, and tool usage. The following examples demonstrate powerful techniques for creating specialized, reliable agents.\n\n**Best Practices & Examples:**\n\n*   **Be Specific & Concise**: Avoid ambiguity.\n*   **Define Persona & Role**: Give the LLM a clear role.\n*   **Constrain Behavior & Tool Use**: Explicitly state what the LLM should *and should not* do.\n*   **Define Output Format**: Tell the LLM *exactly* what its output should look like, especially when not using `output_schema`.\n*   **Dynamic Injection**: Use `{state_key}` to inject runtime data from `session.state` into the prompt.\n*   **Iteration**: Test, observe, and refine instructions.\n\n**Example 1: Constraining Tool Use and Output Format**\n```python\nimport datetime\nfrom google.adk.tools import google_search   \n\n\nplan_generator = LlmAgent(\n    model=\"gemini-2.5-flash\",\n    name=\"plan_generator\",\n    description=\"Generates a 4-5 line action-oriented research plan.\",\n    instruction=f\"\"\"\n    You are a research strategist. Your job is to create a high-level RESEARCH PLAN, not a summary.\n    **RULE: Your output MUST be a bulleted list of 4-5 action-oriented research goals or key questions.**\n    - A good goal starts with a verb like \"Analyze,\" \"Identify,\" \"Investigate.\"\n    - A bad output is a statement of fact like \"The event was in April 2024.\"\n    **TOOL USE IS STRICTLY LIMITED:**\n    Your goal is to create a generic, high-quality plan *without searching*.\n    Only use `google_search` if a topic is ambiguous and you absolutely cannot create a plan without it.\n    You are explicitly forbidden from researching the *content* or *themes* of the topic.\n    Current date: {datetime.datetime.now().strftime(\"%Y-%m-%d\")}\n    \"\"\",\n    tools=[google_search],\n)\n```\n\n**Example 2: Injecting Data from State and Specifying Custom Tags**\nThis agent's `instruction` relies on data placed in `session.state` by previous agents.\n```python\nreport_composer = LlmAgent(\n    model=\"gemini-2.5-pro\",\n    name=\"report_composer_with_citations\",\n    include_contents=\"none\", # History not needed; all data is injected.\n    description=\"Transforms research data and a markdown outline into a final, cited report.\",\n    instruction=\"\"\"\n    Transform the provided data into a polished, professional, and meticulously cited research report.\n\n    ---\n    ### INPUT DATA\n    *   Research Plan: `{research_plan}`\n    *   Research Findings: `{section_research_findings}`\n    *   Citation Sources: `{sources}`\n    *   Report Structure: `{report_sections}`\n\n    ---\n    ### CRITICAL: Citation System\n    To cite a source, you MUST insert a special citation tag directly after the claim it supports.\n\n    **The only correct format is:** `<cite source=\"src-ID_NUMBER\" />`\n\n    ---\n    ### Final Instructions\n    Generate a comprehensive report using ONLY the `<cite source=\"src-ID_NUMBER\" />` tag system for all citations.\n    The final report must strictly follow the structure provided in the **Report Structure** markdown outline.\n    Do not include a \"References\" or \"Sources\" section; all citations must be in-line.\n    \"\"\",\n    output_key=\"final_cited_report\",\n)\n```\n\n---\n\n## 3. Orchestration with Workflow Agents\n\nWorkflow agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) provide deterministic control flow, combining LLM capabilities with structured execution. They do **not** use an LLM for their own orchestration logic.\n\n### 3.1 `SequentialAgent`: Linear Execution\n\nExecutes `sub_agents` one after another in the order defined. The `InvocationContext` is passed along, allowing state changes to be visible to subsequent agents.\n\n```python\nfrom google.adk.agents import SequentialAgent, Agent\n\n# Agent 1: Summarizes a document and saves to state\nsummarizer = Agent(\n    name=\"DocumentSummarizer\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"Summarize the provided document in 3 sentences.\",\n    output_key=\"document_summary\" # Output saved to session.state['document_summary']\n)\n\n# Agent 2: Generates questions based on the summary from state\nquestion_generator = Agent(\n    name=\"QuestionGenerator\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"Generate 3 comprehension questions based on this summary: {document_summary}\",\n    # 'document_summary' is dynamically injected from session.state\n)\n\ndocument_pipeline = SequentialAgent(\n    name=\"SummaryQuestionPipeline\",\n    sub_agents=[summarizer, question_generator], # Order matters!\n    description=\"Summarizes a document then generates questions.\"\n)\n```\n\n### 3.2 `ParallelAgent`: Concurrent Execution\n\nExecutes `sub_agents` simultaneously. Useful for independent tasks to reduce overall latency. All sub-agents share the same `session.state`.\n\n```python\nfrom google.adk.agents import ParallelAgent, Agent, SequentialAgent\n\n# Agents to fetch data concurrently\nfetch_stock_price = Agent(name=\"StockPriceFetcher\", ..., output_key=\"stock_data\")\nfetch_news_headlines = Agent(name=\"NewsFetcher\", ..., output_key=\"news_data\")\nfetch_social_sentiment = Agent(name=\"SentimentAnalyzer\", ..., output_key=\"sentiment_data\")\n\n# Agent to merge results (runs after ParallelAgent, usually in a SequentialAgent)\nmerger_agent = Agent(\n    name=\"ReportGenerator\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"Combine stock data: {stock_data}, news: {news_data}, and sentiment: {sentiment_data} into a market report.\"\n)\n\n# Pipeline to run parallel fetching then sequential merging\nmarket_analysis_pipeline = SequentialAgent(\n    name=\"MarketAnalyzer\",\n    sub_agents=[\n        ParallelAgent(\n            name=\"ConcurrentFetch\",\n            sub_agents=[fetch_stock_price, fetch_news_headlines, fetch_social_sentiment]\n        ),\n        merger_agent # Runs after all parallel agents complete\n    ]\n)\n```\n*   **Concurrency Caution**: When parallel agents write to the same `state` key, race conditions can occur. Always use distinct `output_key`s or manage concurrent writes explicitly.\n\n### 3.3 `LoopAgent`: Iterative Processes\n\nRepeatedly executes its `sub_agents` (sequentially within each loop iteration) until a condition is met or `max_iterations` is reached.\n\n#### **Termination of `LoopAgent`**\nA `LoopAgent` terminates when:\n1.  `max_iterations` is reached.\n2.  Any `Event` yielded by a sub-agent (or a tool within it) sets `actions.escalate = True`. This provides dynamic, content-driven loop termination.\n\n#### **Example: Iterative Refinement Loop with a Custom `BaseAgent` for Control**\nThis example shows a loop that continues until a condition, determined by an evaluation agent, is met.\n\n```python\nfrom google.adk.agents import LoopAgent, Agent, BaseAgent\nfrom google.adk.events import Event, EventActions\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom typing import AsyncGenerator\n\n# An LLM Agent that evaluates research and produces structured JSON output\nresearch_evaluator = Agent(\n    name=\"research_evaluator\",\n    # ... configuration from Section 2.2 ...\n    output_schema=Feedback,\n    output_key=\"research_evaluation\",\n)\n\n# An LLM Agent that performs additional searches based on feedback\nenhanced_search_executor = Agent(\n    name=\"enhanced_search_executor\",\n    instruction=\"Execute the follow-up queries from 'research_evaluation' and combine with existing findings.\",\n    # ... other configurations ...\n)\n\n# A custom BaseAgent to check the evaluation and stop the loop\nclass EscalationChecker(BaseAgent):\n    \"\"\"Checks research evaluation and escalates to stop the loop if grade is 'pass'.\"\"\"\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\n        evaluation = ctx.session.state.get(\"research_evaluation\")\n        if evaluation and evaluation.get(\"grade\") == \"pass\":\n            # The key to stopping the loop: yield an Event with escalate=True\n            yield Event(author=self.name, actions=EventActions(escalate=True))\n        else:\n            # Let the loop continue\n            yield Event(author=self.name)\n\n# Define the loop\niterative_refinement_loop = LoopAgent(\n    name=\"IterativeRefinementLoop\",\n    sub_agents=[\n        research_evaluator, # Step 1: Evaluate\n        EscalationChecker(name=\"EscalationChecker\"), # Step 2: Check and maybe stop\n        enhanced_search_executor, # Step 3: Refine (only runs if loop didn't stop)\n    ],\n    max_iterations=5, # Fallback to prevent infinite loops\n    description=\"Iteratively evaluates and refines research until it passes quality checks.\"\n)\n```\n\n---\n\n## 4. Multi-Agent Systems & Communication\n\nBuilding complex applications by composing multiple, specialized agents.\n\n### 4.1 Agent Hierarchy\n\nA hierarchical (tree-like) structure of parent-child relationships defined by the `sub_agents` parameter during `BaseAgent` initialization. An agent can only have one parent.\n\n```python\n# Conceptual Hierarchy\n# Root\n# \u2514\u2500\u2500 Coordinator (LlmAgent)\n#     \u251c\u2500\u2500 SalesAgent (LlmAgent)\n#     \u2514\u2500\u2500 SupportAgent (LlmAgent)\n#     \u2514\u2500\u2500 DataPipeline (SequentialAgent)\n#         \u251c\u2500\u2500 DataFetcher (LlmAgent)\n#         \u2514\u2500\u2500 DataProcessor (LlmAgent)\n```\n\n### 4.2 Inter-Agent Communication Mechanisms\n\n1.  **Shared Session State (`session.state`)**: The most common and robust method. Agents read from and write to the same mutable dictionary.\n    *   **Mechanism**: Agent A sets `ctx.session.state['key'] = value`. Agent B later reads `ctx.session.state.get('key')`. `output_key` on `LlmAgent` is a convenient auto-setter.\n    *   **Best for**: Passing intermediate results, shared configurations, and flags in pipelines (Sequential, Loop agents).\n\n2.  **LLM-Driven Delegation (`transfer_to_agent`)**: A `LlmAgent` can dynamically hand over control to another agent based on its reasoning.\n    *   **Mechanism**: The LLM generates a special `transfer_to_agent` function call. The ADK framework intercepts this, routes the next turn to the target agent.\n    *   **Prerequisites**:\n        *   The initiating `LlmAgent` needs `instruction` to guide delegation and `description` of the target agent(s).\n        *   Target agents need clear `description`s to help the LLM decide.\n        *   Target agent must be discoverable within the current agent's hierarchy (direct `sub_agent` or a descendant).\n    *   **Configuration**: Can be enabled/disabled via `disallow_transfer_to_parent` and `disallow_transfer_to_peers` on `LlmAgent`.\n\n3.  **Explicit Invocation (`AgentTool`)**: An `LlmAgent` can treat another `BaseAgent` instance as a callable tool.\n    *   **Mechanism**: Wrap the target agent (`target_agent`) in `AgentTool(agent=target_agent)` and add it to the calling `LlmAgent`'s `tools` list. The `AgentTool` generates a `FunctionDeclaration` for the LLM. When called, `AgentTool` runs the target agent and returns its final response as the tool result.\n    *   **Best for**: Hierarchical task decomposition, where a higher-level agent needs a specific output from a lower-level agent.\n\n### 4.3 Common Multi-Agent Patterns\n\n*   **Coordinator/Dispatcher**: A central agent routes requests to specialized sub-agents (often via LLM-driven delegation).\n*   **Sequential Pipeline**: `SequentialAgent` orchestrates a fixed sequence of tasks, passing data via shared state.\n*   **Parallel Fan-Out/Gather**: `ParallelAgent` runs concurrent tasks, followed by a final agent that synthesizes results from state.\n*   **Review/Critique (Generator-Critic)**: `SequentialAgent` with a generator followed by a critic, often in a `LoopAgent` for iterative refinement.\n*   **Hierarchical Task Decomposition (Planner/Executor)**: High-level agents break down complex problems, delegating sub-tasks to lower-level agents (often via `AgentTool` and delegation).\n\n#### **Example: Hierarchical Planner/Executor Pattern**\nThis pattern combines several mechanisms. A top-level `interactive_planner_agent` uses another agent (`plan_generator`) as a tool to create a plan, then delegates the execution of that plan to a complex `SequentialAgent` (`research_pipeline`).\n\n```python\nfrom google.adk.agents import LlmAgent, SequentialAgent, LoopAgent\nfrom google.adk.tools.agent_tool import AgentTool\n\n# Assume plan_generator, section_planner, research_evaluator, etc. are defined.\n\n# The execution pipeline itself is a complex agent.\nresearch_pipeline = SequentialAgent(\n    name=\"research_pipeline\",\n    description=\"Executes a pre-approved research plan. It performs iterative research, evaluation, and composes a final, cited report.\",\n    sub_agents=[\n        section_planner,\n        section_researcher,\n        LoopAgent(\n            name=\"iterative_refinement_loop\",\n            max_iterations=3,\n            sub_agents=[\n                research_evaluator,\n                EscalationChecker(name=\"escalation_checker\"),\n                enhanced_search_executor,\n            ],\n        ),\n        report_composer,\n    ],\n)\n\n# The top-level agent that interacts with the user.\ninteractive_planner_agent = LlmAgent(\n    name=\"interactive_planner_agent\",\n    model=\"gemini-2.5-flash\",\n    description=\"The primary research assistant. It collaborates with the user to create a research plan, and then executes it upon approval.\",\n    instruction=\"\"\"\n    You are a research planning assistant. Your workflow is:\n    1.  **Plan:** Use the `plan_generator` tool to create a draft research plan.\n    2.  **Refine:** Incorporate user feedback until the plan is approved.\n    3.  **Execute:** Once the user gives EXPLICIT approval (e.g., \"looks good, run it\"), you MUST delegate the task to the `research_pipeline` agent.\n    Your job is to Plan, Refine, and Delegate. Do not do the research yourself.\n    \"\"\",\n    # The planner delegates to the pipeline.\n    sub_agents=[research_pipeline],\n    # The planner uses another agent as a tool.\n    tools=[AgentTool(plan_generator)],\n    output_key=\"research_plan\",\n)\n\n# The root agent of the application is the top-level planner.\nroot_agent = interactive_planner_agent\n```\n\n### 4.A. Distributed Communication (A2A Protocol)\n\nThe Agent-to-Agent (A2A) Protocol enables agents to communicate over a network, even if they are written in different languages or run as separate services. Use A2A for integrating with third-party agents, building microservice-based agent architectures, or when a strong, formal API contract is needed. For internal code organization, prefer local sub-agents.\n\n*   **Exposing an Agent**: Make an existing ADK agent available to others over A2A.\n    *   **`to_a2a()` Utility**: The simplest method. Wraps your `root_agent` and creates a runnable FastAPI app, auto-generating the required `agent.json` card.\n        ```python\n        from google.adk.a2a.utils.agent_to_a2a import to_a2a\n        # root_agent is your existing ADK Agent instance\n        a2a_app = to_a2a(root_agent, port=8001)\n        # Run with: uvicorn your_module:a2a_app --host localhost --port 8001\n        ```\n    *   **`adk api_server --a2a`**: A CLI command that serves agents from a directory. Requires you to manually create an `agent.json` card for each agent you want to expose.\n\n*   **Consuming a Remote Agent**: Use a remote A2A agent as if it were a local agent.\n    *   **`RemoteA2aAgent`**: This agent acts as a client proxy. You initialize it with the URL to the remote agent's card.\n        ```python\n        from google.adk.a2a.remote_a2a_agent import RemoteA2aAgent\n\n        # This agent can now be used as a sub-agent or tool\n        prime_checker_agent = RemoteA2aAgent(\n            name=\"prime_agent\",\n            description=\"A remote agent that checks if numbers are prime.\",\n            agent_card=\"http://localhost:8001/a2a/check_prime_agent/.well-known/agent.json\"\n        )\n        ```\n\n---\n\n## 5. Building Custom Agents (`BaseAgent`)\n\nFor unique orchestration logic that doesn't fit standard workflow agents, inherit directly from `BaseAgent`.\n\n### 5.1 When to Use Custom Agents\n\n*   **Complex Conditional Logic**: `if/else` branching based on multiple state variables.\n*   **Dynamic Agent Selection**: Choosing which sub-agent to run based on runtime evaluation.\n*   **Direct External Integrations**: Calling external APIs or libraries directly within the orchestration flow.\n*   **Custom Loop/Retry Logic**: More sophisticated iteration patterns than `LoopAgent`, such as the `EscalationChecker` example.\n\n### 5.2 Implementing `_run_async_impl`\n\nThis is the core asynchronous method you must override.\n\n#### **Example: A Custom Agent for Loop Control**\nThis agent reads state, applies simple Python logic, and yields an `Event` with an `escalate` action to control a `LoopAgent`.\n\n```python\nfrom google.adk.agents import BaseAgent\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom google.adk.events import Event, EventActions\nfrom typing import AsyncGenerator\nimport logging\n\nclass EscalationChecker(BaseAgent):\n    \"\"\"Checks research evaluation and escalates to stop the loop if grade is 'pass'.\"\"\"\n\n    def __init__(self, name: str):\n        super().__init__(name=name)\n\n    async def _run_async_impl(\n        self, ctx: InvocationContext\n    ) -> AsyncGenerator[Event, None]:\n        # 1. Read from session state.\n        evaluation_result = ctx.session.state.get(\"research_evaluation\")\n\n        # 2. Apply custom Python logic.\n        if evaluation_result and evaluation_result.get(\"grade\") == \"pass\":\n            logging.info(\n                f\"[{self.name}] Research passed. Escalating to stop loop.\"\n            )\n            # 3. Yield an Event with a control Action.\n            yield Event(author=self.name, actions=EventActions(escalate=True))\n        else:\n            logging.info(\n                f\"[{self.name}] Research failed or not found. Loop continues.\"\n            )\n            # Yielding an event without actions lets the flow continue.\n            yield Event(author=self.name)\n```\n*   **Asynchronous Generator**: `async def ... yield Event`. This allows pausing and resuming execution.\n*   **`ctx: InvocationContext`**: Provides access to all session state (`ctx.session.state`).\n*   **Calling Sub-Agents**: Use `async for event in self.sub_agent_instance.run_async(ctx): yield event`.\n*   **Control Flow**: Use standard Python `if/else`, `for/while` loops for complex logic.\n\n---\n\n## 6. Models: Gemini, LiteLLM, and Vertex AI\n\nADK's model flexibility allows integrating various LLMs for different needs.\n\n### 6.1 Google Gemini Models (AI Studio & Vertex AI)\n\n*   **Default Integration**: Native support via `google-genai` library.\n*   **AI Studio (Easy Start)**:\n    *   Set `GOOGLE_API_KEY=\"YOUR_API_KEY\"` (environment variable).\n    *   Set `GOOGLE_GENAI_USE_VERTEXAI=\"False\"`.\n    *   Model strings: `\"gemini-2.5-flash\"`, `\"gemini-2.5-pro\"`, etc.\n*   **Vertex AI (Production)**:\n    *   Authenticate via `gcloud auth application-default login` (recommended).\n    *   Set `GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\"`, `GOOGLE_CLOUD_LOCATION=\"your-region\"` (environment variables).\n    *   Set `GOOGLE_GENAI_USE_VERTEXAI=\"True\"`.\n    *   Model strings: `\"gemini-2.5-flash\"`, `\"gemini-2.5-pro\"`, or full Vertex AI endpoint resource names for specific deployments.\n\n### 6.2 Other Cloud & Proprietary Models via LiteLLM\n\n`LiteLlm` provides a unified interface to 100+ LLMs (OpenAI, Anthropic, Cohere, etc.).\n\n*   **Installation**: `pip install litellm`\n*   **API Keys**: Set environment variables as required by LiteLLM (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`).\n*   **Usage**:\n    ```python\n    from google.adk.models.lite_llm import LiteLlm\n    agent_openai = Agent(model=LiteLlm(model=\"openai/gpt-4o\"), ...)\n    agent_claude = Agent(model=LiteLlm(model=\"anthropic/claude-3-haiku-20240307\"), ...)\n    ```\n\n### 6.3 Open & Local Models via LiteLLM (Ollama, vLLM)\n\nFor self-hosting, cost savings, privacy, or offline use.\n\n*   **Ollama Integration**: Run Ollama locally (`ollama run <model>`).\n    ```bash\n    export OLLAMA_API_BASE=\"http://localhost:11434\" # Ensure Ollama server is running\n    ```\n    ```python\n    from google.adk.models.lite_llm import LiteLlm\n    # Use 'ollama_chat' provider for tool-calling capabilities with Ollama models\n    agent_ollama = Agent(model=LiteLlm(model=\"ollama_chat/llama3:instruct\"), ...)\n    ```\n\n*   **Self-Hosted Endpoint (e.g., vLLM)**:\n    ```python\n    from google.adk.models.lite_llm import LiteLlm\n    api_base_url = \"https://your-vllm-endpoint.example.com/v1\"\n    agent_vllm = Agent(\n        model=LiteLlm(\n            model=\"your-model-name-on-vllm\",\n            api_base=api_base_url,\n            extra_headers={\"Authorization\": \"Bearer YOUR_TOKEN\"},\n        ),\n        ...\n    )\n    ```\n\n### 6.4 Customizing LLM API Clients\n\nFor `google-genai` (used by Gemini models), you can configure the underlying client.\n\n```python\nimport os\nfrom google.genai import configure as genai_configure\n\ngenai_configure.use_defaults(\n    timeout=60, # seconds\n    client_options={\"api_key\": os.getenv(\"GOOGLE_API_KEY\")},\n)\n```\n\n---\n\n## 7. Tools: The Agent's Capabilities\n\nTools extend an agent's abilities beyond text generation.\n\n### 7.1 Defining Function Tools: Principles & Best Practices\n\n*   **Signature**: `def my_tool(param1: Type, param2: Type, tool_context: ToolContext) -> dict:`\n*   **Function Name**: Descriptive verb-noun (e.g., `schedule_meeting`).\n*   **Parameters**: Clear names, required type hints, **NO DEFAULT VALUES**.\n*   **Return Type**: **Must** be a `dict` (JSON-serializable), preferably with a `'status'` key.\n*   **Docstring**: **CRITICAL**. Explain purpose, when to use, arguments, and return value structure. **AVOID** mentioning `tool_context`.\n\n    ```python\n    def calculate_compound_interest(\n        principal: float,\n        rate: float,\n        years: int,\n        compounding_frequency: int,\n        tool_context: ToolContext\n    ) -> dict:\n        \"\"\"Calculates the future value of an investment with compound interest.\n\n        Use this tool to calculate the future value of an investment given a\n        principal amount, interest rate, number of years, and how often the\n        interest is compounded per year.\n\n        Args:\n            principal (float): The initial amount of money invested.\n            rate (float): The annual interest rate (e.g., 0.05 for 5%).\n            years (int): The number of years the money is invested.\n            compounding_frequency (int): The number of times interest is compounded\n                                         per year (e.g., 1 for annually, 12 for monthly).\n            \n        Returns:\n            dict: Contains the calculation result.\n                  - 'status' (str): \"success\" or \"error\".\n                  - 'future_value' (float, optional): The calculated future value.\n                  - 'error_message' (str, optional): Description of error, if any.\n        \"\"\"\n        # ... implementation ...\n    ```\n\n### 7.2 The `ToolContext` Object: Accessing Runtime Information\n\n`ToolContext` is the gateway for tools to interact with the ADK runtime.\n\n*   `tool_context.state`: Read and write to the current `Session`'s `state` dictionary.\n*   `tool_context.actions`: Modify the `EventActions` object (e.g., `tool_context.actions.escalate = True`).\n*   `tool_context.load_artifact(filename)` / `tool_context.save_artifact(filename, part)`: Manage binary data.\n*   `tool_context.search_memory(query)`: Query the long-term `MemoryService`.\n\n### 7.3 All Tool Types & Their Usage\n\n1.  **Custom Function Tools**:\n    *   **`FunctionTool`**: The most common type, wrapping a standard Python function.\n    *   **`LongRunningFunctionTool`**: Wraps an `async` function that `yields` intermediate results, for tasks that provide progress updates.\n    *   **`AgentTool`**: Wraps another `BaseAgent` instance, allowing it to be invoked as a tool by a parent agent.\n\n2.  **Built-in Tools**: Ready-to-use tools provided by ADK.\n    *   `google_search`: Provides Google Search grounding.\n    *   `BuiltInCodeExecutor`: Enables sandboxed code execution.\n    *   `VertexAiSearchTool`: Provides grounding from your private Vertex AI Search data stores.\n    *   `BigQueryToolset`: A collection of tools for interacting with BigQuery (e.g., `list_datasets`, `execute_sql`).\n    > **Warning**: An agent can only use one type of built-in tool at a time and they cannot be used in sub-agents.\n\n3.  **Third-Party Tool Wrappers**: For seamless integration with other frameworks.\n    *   `LangchainTool`: Wraps a tool from the LangChain ecosystem.\n    *   `CrewaiTool`: Wraps a tool from the CrewAI library.\n\n4.  **OpenAPI & Protocol Tools**: For interacting with APIs and services.\n    *   **`OpenAPIToolset`**: Automatically generates a set of `RestApiTool`s from an OpenAPI (Swagger) v3 specification.\n    *   **`MCPToolset`**: Connects to an external Model Context Protocol (MCP) server to dynamically load its tools.\n\n5.  **Google Cloud Tools**: For deep integration with Google Cloud services.\n    *   **`ApiHubToolset`**: Turns any documented API from Apigee API Hub into a tool.\n    *   **`ApplicationIntegrationToolset`**: Turns Application Integration workflows and Integration Connectors (e.g., Salesforce, SAP) into callable tools.\n    *   **Toolbox for Databases**: An open-source MCP server that ADK can connect to for database interactions.\n\n---\n\n## 8. Context, State, and Memory Management\n\nEffective context management is crucial for coherent, multi-turn conversations.\n\n### 8.1 The `Session` Object & `SessionService`\n\n*   **`Session`**: The container for a single, ongoing conversation (`id`, `state`, `events`).\n*   **`SessionService`**: Manages the lifecycle of `Session` objects (`create_session`, `get_session`, `append_event`).\n*   **Implementations**: `InMemorySessionService` (dev), `VertexAiSessionService` (prod), `DatabaseSessionService` (self-managed).\n\n### 8.2 `State`: The Conversational Scratchpad\n\nA mutable dictionary within `session.state` for short-term, dynamic data.\n\n*   **Update Mechanism**: Always update via `context.state` (in callbacks/tools) or `LlmAgent.output_key`.\n*   **Prefixes for Scope**:\n    *   **(No prefix)**: Session-specific (e.g., `session.state['booking_step']`).\n    *   `user:`: Persistent for a `user_id` across all their sessions (e.g., `session.state['user:preferred_currency']`).\n    *   `app:`: Persistent for `app_name` across all users and sessions.\n    *   `temp:`: Volatile, for the current `Invocation` turn only.\n\n### 8.3 `Memory`: Long-Term Knowledge & Retrieval\n\nFor knowledge beyond a single conversation.\n\n*   **`BaseMemoryService`**: Defines the interface (`add_session_to_memory`, `search_memory`).\n*   **Implementations**: `InMemoryMemoryService`, `VertexAiRagMemoryService`.\n*   **Usage**: Agents interact via tools (e.g., the built-in `load_memory` tool).\n\n### 8.4 `Artifacts`: Binary Data Management\n\nFor named, versioned binary data (files, images).\n\n*   **Representation**: `google.genai.types.Part` (containing a `Blob` with `data: bytes` and `mime_type: str`).\n*   **`BaseArtifactService`**: Manages storage (`save_artifact`, `load_artifact`).\n*   **Implementations**: `InMemoryArtifactService`, `GcsArtifactService`.\n\n---\n\n## 9. Runtime, Events, and Execution Flow\n\nThe `Runner` is the central orchestrator of an ADK application.\n\n### 9.1 The `Runner`: The Orchestrator\n\n*   **Role**: Manages the agent's lifecycle, the event loop, and coordinates with services.\n*   **Entry Point**: `runner.run_async(user_id, session_id, new_message)`.\n\n### 9.2 The Event Loop: Core Execution Flow\n\n1.  User input becomes a `user` `Event`.\n2.  `Runner` calls `agent.run_async(invocation_context)`.\n3.  Agent `yield`s an `Event` (e.g., tool call, text response). Execution pauses.\n4.  `Runner` processes the `Event` (applies state changes, etc.) and yields it to the client.\n5.  Execution resumes. This cycle repeats until the agent is done.\n\n### 9.3 `Event` Object: The Communication Backbone\n\n`Event` objects carry all information and signals.\n\n*   `Event.author`: Source of the event (`'user'`, agent name, `'system'`).\n*   `Event.content`: The primary payload (text, function calls, function responses).\n*   `Event.actions`: Signals side effects (`state_delta`, `transfer_to_agent`, `escalate`).\n*   `Event.is_final_response()`: Helper to identify the complete, displayable message.\n\n### 9.4 Asynchronous Programming (Python Specific)\n\nADK is built on `asyncio`. Use `async def`, `await`, and `async for` for all I/O-bound operations.\n\n---\n\n## 10. Control Flow with Callbacks\n\nCallbacks are functions that intercept and control agent execution at specific points.\n\n### 10.1 Callback Mechanism: Interception & Control\n\n*   **Definition**: A Python function assigned to an agent's `callback` parameter (e.g., `after_agent_callback=my_func`).\n*   **Context**: Receives a `CallbackContext` (or `ToolContext`) with runtime info.\n*   **Return Value**: **Crucially determines flow.**\n    *   `return None`: Allow the default action to proceed.\n    *   `return <Specific Object>`: **Override** the default action/result.\n\n### 10.2 Types of Callbacks\n\n1.  **Agent Lifecycle**: `before_agent_callback`, `after_agent_callback`.\n2.  **LLM Interaction**: `before_model_callback`, `after_model_callback`.\n3.  **Tool Execution**: `before_tool_callback`, `after_tool_callback`.\n\n### 10.3 Callback Best Practices\n\n*   **Keep Focused**: Each callback for a single purpose.\n*   **Performance**: Avoid blocking I/O or heavy computation.\n*   **Error Handling**: Use `try...except` to prevent crashes.\n\n#### **Example 1: Data Aggregation with `after_agent_callback`**\nThis callback runs after an agent, inspects the `session.events` to find structured data from tool calls (like `google_search` results), and saves it to state for later use.\n\n```python\nfrom google.adk.agents.callback_context import CallbackContext\n\ndef collect_research_sources_callback(callback_context: CallbackContext) -> None:\n    \"\"\"Collects and organizes web research sources from agent events.\"\"\"\n    session = callback_context._invocation_context.session\n    # Get existing sources from state to append to them.\n    url_to_short_id = callback_context.state.get(\"url_to_short_id\", {})\n    sources = callback_context.state.get(\"sources\", {})\n    id_counter = len(url_to_short_id) + 1\n\n    # Iterate through all events in the session to find grounding metadata.\n    for event in session.events:\n        if not (event.grounding_metadata and event.grounding_metadata.grounding_chunks):\n            continue\n        # ... logic to parse grounding_chunks and grounding_supports ...\n        # (See full implementation in the original code snippet)\n\n    # Save the updated source map back to state.\n    callback_context.state[\"url_to_short_id\"] = url_to_short_id\n    callback_context.state[\"sources\"] = sources\n\n# Used in an agent like this:\n# section_researcher = LlmAgent(..., after_agent_callback=collect_research_sources_callback)\n```\n\n#### **Example 2: Output Transformation with `after_agent_callback`**\nThis callback takes an LLM's raw output (containing custom tags), uses Python to format it into markdown, and returns the modified content, overriding the original.\n\n```python\nimport re\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.genai import types as genai_types\n\ndef citation_replacement_callback(callback_context: CallbackContext) -> genai_types.Content:\n    \"\"\"Replaces <cite> tags in a report with Markdown-formatted links.\"\"\"\n    # 1. Get raw report and sources from state.\n    final_report = callback_context.state.get(\"final_cited_report\", \"\")\n    sources = callback_context.state.get(\"sources\", {})\n\n    # 2. Define a replacer function for regex substitution.\n    def tag_replacer(match: re.Match) -> str:\n        short_id = match.group(1)\n        if not (source_info := sources.get(short_id)):\n            return \"\" # Remove invalid tags\n        title = source_info.get(\"title\", short_id)\n        return f\" [{title}]({source_info['url']})\"\n\n    # 3. Use regex to find all <cite> tags and replace them.\n    processed_report = re.sub(\n        r'<cite\\s+source\\s*=\\s*[\"\\']?(src-\\d+)[\"\\']?\\s*/>',\n        tag_replacer,\n        final_report,\n    )\n    processed_report = re.sub(r\"\\s+([.,;:])\", r\"\\1\", processed_report) # Fix spacing\n\n    # 4. Save the new version to state and return it to override the original agent output.\n    callback_context.state[\"final_report_with_citations\"] = processed_report\n    return genai_types.Content(parts=[genai_types.Part(text=processed_report)])\n\n# Used in an agent like this:\n# report_composer = LlmAgent(..., after_agent_callback=citation_replacement_callback)\n```\n\n### 10.A. Global Control with Plugins\n\nPlugins are stateful, reusable modules for implementing cross-cutting concerns that apply globally to all agents, tools, and model calls managed by a `Runner`. Unlike Callbacks which are configured per-agent, Plugins are registered once on the `Runner`.\n\n*   **Use Cases**: Ideal for universal logging, application-wide policy enforcement, global caching, and collecting metrics.\n*   **Execution Order**: Plugin callbacks run **before** their corresponding agent-level callbacks. If a plugin callback returns a value, the agent-level callback is skipped.\n*   **Defining a Plugin**: Inherit from `BasePlugin` and implement callback methods.\n    ```python\n    from google.adk.plugins.base_plugin import BasePlugin\n    from google.adk.agents.callback_context import CallbackContext\n\n    class InvocationCounterPlugin(BasePlugin):\n        def __init__(self):\n            super().__init__(name=\"invocation_counter\")\n            self.agent_runs = 0\n\n        async def before_agent_callback(self, callback_context: CallbackContext, **kwargs):\n            self.agent_runs += 1\n            print(f\"[Plugin] Total agent runs: {self.agent_runs}\")\n    ```\n*   **Registering a Plugin**:\n    ```python\n    from google.adk.runners import Runner\n    # runner = Runner(agent=root_agent, ..., plugins=[InvocationCounterPlugin()])\n    ```\n*   **Error Handling Callbacks**: Plugins support unique error hooks like `on_model_error_callback` and `on_tool_error_callback` for centralized error management.\n*   **Limitation**: Plugins are not supported by the `adk web` interface.\n\n---\n\n## 11. Authentication for Tools\n\nEnabling agents to securely access protected external resources.\n\n### 11.1 Core Concepts: `AuthScheme` & `AuthCredential`\n\n*   **`AuthScheme`**: Defines *how* an API expects authentication (e.g., `APIKey`, `HTTPBearer`, `OAuth2`, `OpenIdConnectWithConfig`).\n*   **`AuthCredential`**: Holds *initial* information to *start* the auth process (e.g., API key value, OAuth client ID/secret).\n\n### 11.2 Interactive OAuth/OIDC Flows\n\nWhen a tool requires user interaction (OAuth consent), ADK pauses and signals your `Agent Client` application.\n\n1.  **Detect Auth Request**: `runner.run_async()` yields an event with a special `adk_request_credential` function call.\n2.  **Redirect User**: Extract `auth_uri` from `auth_config` in the event. Your client app redirects the user's browser to this `auth_uri` (appending `redirect_uri`).\n3.  **Handle Callback**: Your client app has a pre-registered `redirect_uri` to receive the user after authorization. It captures the full callback URL (containing `authorization_code`).\n4.  **Send Auth Result to ADK**: Your client prepares a `FunctionResponse` for `adk_request_credential`, setting `auth_config.exchanged_auth_credential.oauth2.auth_response_uri` to the captured callback URL.\n5.  **Resume Execution**: `runner.run_async()` is called again with this `FunctionResponse`. ADK performs the token exchange, stores the access token, and retries the original tool call.\n\n### 11.3 Custom Tool Authentication\n\nIf building a `FunctionTool` that needs authentication:\n\n1.  **Check for Cached Creds**: `tool_context.state.get(\"my_token_cache_key\")`.\n2.  **Check for Auth Response**: `tool_context.get_auth_response(my_auth_config)`.\n3.  **Initiate Auth**: If no creds, call `tool_context.request_credential(my_auth_config)` and return a pending status. This triggers the external flow.\n4.  **Cache Credentials**: After obtaining, store in `tool_context.state`.\n5.  **Make API Call**: Use the valid credentials (e.g., `google.oauth2.credentials.Credentials`).\n\n---\n\n## 12. Deployment Strategies\n\nFrom local dev to production.\n\n### 12.1 Local Development & Testing (`adk web`, `adk run`, `adk api_server`)\n\n*   **`adk web`**: Launches a local web UI for interactive chat, session inspection, and visual tracing.\n    ```bash\n    adk web /path/to/your/project_root\n    ```\n*   **`adk run`**: Command-line interactive chat.\n    ```bash\n    adk run /path/to/your/agent_folder\n    ```\n*   **`adk api_server`**: Launches a local FastAPI server exposing `/run`, `/run_sse`, `/list-apps`, etc., for API testing with `curl` or client libraries.\n    ```bash\n    adk api_server /path/to/your/project_root\n    ```\n\n### 12.2 Vertex AI Agent Engine\n\nFully managed, scalable service for ADK agents on Google Cloud.\n\n*   **Features**: Auto-scaling, session management, observability integration.\n*   **ADK CLI**: `adk deploy agent_engine --project <id> --region <loc> ... /path/to/agent`\n*   **Deployment**: Use `vertexai.agent_engines.create()`.\n    ```python\n    from vertexai.preview import reasoning_engines # or agent_engines directly in later versions\n    \n    # Wrap your root_agent for deployment\n    app_for_engine = reasoning_engines.AdkApp(agent=root_agent, enable_tracing=True)\n    \n    # Deploy\n    remote_app = agent_engines.create(\n        agent_engine=app_for_engine,\n        requirements=[\"google-cloud-aiplatform[adk,agent_engines]\"],\n        display_name=\"My Production Agent\"\n    )\n    print(remote_app.resource_name) # projects/PROJECT_NUM/locations/REGION/reasoningEngines/ID\n    ```\n*   **Interaction**: Use `remote_app.stream_query()`, `create_session()`, etc.\n\n### 12.3 Cloud Run\n\nServerless container platform for custom web applications.\n\n*   **ADK CLI**: `adk deploy cloud_run --project <id> --region <loc> ... /path/to/agent`\n*   **Deployment**:\n    1.  Create a `Dockerfile` for your FastAPI app (using `google.adk.cli.fast_api.get_fast_api_app`).\n    2.  Use `gcloud run deploy --source .`.\n    3.  Alternatively, `adk deploy cloud_run` (simpler, opinionated).\n*   **Example `main.py`**:\n    ```python\n    import os\n    from fastapi import FastAPI\n    from google.adk.cli.fast_api import get_fast_api_app\n\n    # Ensure your agent_folder (e.g., 'my_first_agent') is in the same directory as main.py\n    app: FastAPI = get_fast_api_app(\n        agents_dir=os.path.dirname(os.path.abspath(__file__)),\n        session_service_uri=\"sqlite:///./sessions.db\", # In-container SQLite, for simple cases\n        # For production: use a persistent DB (Cloud SQL) or VertexAiSessionService\n        allow_origins=[\"*\"],\n        web=True # Serve ADK UI\n    )\n    # uvicorn.run(app, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080))) # If running directly\n    ```\n\n### 12.4 Google Kubernetes Engine (GKE)\n\nFor maximum control, run your containerized agent in a Kubernetes cluster.\n\n*   **ADK CLI**: `adk deploy gke --project <id> --cluster_name <name> ... /path/to/agent`\n*   **Deployment**:\n    1.  Build Docker image (`gcloud builds submit`).\n    2.  Create Kubernetes Deployment and Service YAMLs.\n    3.  Apply with `kubectl apply -f deployment.yaml`.\n    4.  Configure Workload Identity for GCP permissions.\n\n### 12.5 CI/CD Integration\n\n*   Automate testing (`pytest`, `adk eval`) in CI.\n*   Automate container builds and deployments (e.g., Cloud Build, GitHub Actions).\n*   Use environment variables for secrets.\n\n---\n\n## 13. Evaluation and Safety\n\nCritical for robust, production-ready agents.\n\n### 13.1 Agent Evaluation (`adk eval`)\n\nSystematically assess agent performance using predefined test cases.\n\n*   **Evalset File (`.evalset.json`)**: Contains `eval_cases`, each with a `conversation` (user queries, expected tool calls, expected intermediate/final responses) and `session_input` (initial state).\n    ```json\n    {\n      \"eval_set_id\": \"weather_bot_eval\",\n      \"eval_cases\": [\n        {\n          \"eval_id\": \"london_weather_query\",\n          \"conversation\": [\n            {\n              \"user_content\": {\"parts\": [{\"text\": \"What's the weather in London?\"}]},\n              \"final_response\": {\"parts\": [{\"text\": \"The weather in London is cloudy...\"}]},\n              \"intermediate_data\": {\n                \"tool_uses\": [{\"name\": \"get_weather\", \"args\": {\"city\": \"London\"}}]\n              }\n            }\n          ],\n          \"session_input\": {\"app_name\": \"weather_app\", \"user_id\": \"test_user\", \"state\": {}}\n        }\n      ]\n    }\n    ```\n*   **Running Evaluation**:\n    *   `adk web`: Interactive UI for creating/running eval cases.\n    *   `adk eval /path/to/agent_folder /path/to/evalset.json`: CLI execution.\n    *   `pytest`: Integrate `AgentEvaluator.evaluate()` into unit/integration tests.\n*   **Metrics**: `tool_trajectory_avg_score` (tool calls match expected), `response_match_score` (final response similarity using ROUGE). Configurable via `test_config.json`.\n\n### 13.2 Safety & Guardrails\n\nMulti-layered defense against harmful content, misalignment, and unsafe actions.\n\n1.  **Identity and Authorization**:\n    *   **Agent-Auth**: Tool acts with the agent's service account (e.g., `Vertex AI User` role). Simple, but all users share access level. Logs needed for attribution.\n    *   **User-Auth**: Tool acts with the end-user's identity (via OAuth tokens). Reduces risk of abuse.\n2.  **In-Tool Guardrails**: Design tools defensively. Tools can read policies from `tool_context.state` (set deterministically by developer) and validate model-provided arguments before execution.\n    ```python\n    def execute_sql(query: str, tool_context: ToolContext) -> dict:\n        policy = tool_context.state.get(\"user:sql_policy\", {})\n        if not policy.get(\"allow_writes\", False) and (\"INSERT\" in query.upper() or \"DELETE\" in query.upper()):\n            return {\"status\": \"error\", \"message\": \"Policy: Write operations are not allowed.\"}\n        # ... execute query ...\n    ```\n3.  **Built-in Gemini Safety Features**:\n    *   **Content Safety Filters**: Automatically block harmful content (CSAM, PII, hate speech, etc.). Configurable thresholds.\n    *   **System Instructions**: Guide model behavior, define prohibited topics, brand tone, disclaimers.\n4.  **Model and Tool Callbacks (LLM as a Guardrail)**: Use callbacks to inspect inputs/outputs.\n    *   `before_model_callback`: Intercept `LlmRequest` before it hits the LLM. Block (return `LlmResponse`) or modify.\n    *   `before_tool_callback`: Intercept tool calls (name, args) before execution. Block (return `dict`) or modify.\n    *   **LLM-based Safety**: Use a cheap/fast LLM (e.g., Gemini Flash) in a callback to classify input/output safety.\n        ```python\n        def safety_checker_callback(context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n            # Use a separate, small LLM to classify safety\n            safety_llm_agent = Agent(name=\"SafetyChecker\", model=\"gemini-2.5-flash-001\", instruction=\"Classify input as 'safe' or 'unsafe'. Output ONLY the word.\")\n            # Run the safety agent (might need a new runner instance or direct model call)\n            # For simplicity, a mock:\n            user_input = llm_request.contents[-1].parts[0].text\n            if \"dangerous_phrase\" in user_input.lower():\n                context.state[\"safety_violation\"] = True\n                return LlmResponse(content=genai_types.Content(parts=[genai_types.Part(text=\"I cannot process this request due to safety concerns.\")]))\n            return None\n        ```\n5.  **Sandboxed Code Execution**:\n    *   `BuiltInCodeExecutor`: Uses secure, sandboxed execution environments.\n    *   Vertex AI Code Interpreter Extension.\n    *   If custom, ensure hermetic environments (no network, isolated).\n6.  **Network Controls & VPC-SC**: Confine agent activity within secure perimeters (VPC Service Controls) to prevent data exfiltration.\n7.  **Output Escaping in UIs**: Always properly escape LLM-generated content in web UIs to prevent XSS attacks and indirect prompt injections.\n\n**Grounding**: A key safety and reliability feature that connects agent responses to verifiable information.\n*   **Mechanism**: Uses tools like `google_search` or `VertexAiSearchTool` to fetch real-time or private data.\n*   **Benefit**: Reduces model hallucination by basing responses on retrieved facts.\n*   **Requirement**: When using `google_search`, your application UI **must** display the provided search suggestions and citations to comply with terms of service.\n\n---\n\n## 14. Debugging, Logging & Observability\n\n*   **`adk web` UI**: Best first step. Provides visual trace, session history, and state inspection.\n*   **Event Stream Logging**: Iterate `runner.run_async()` events and print relevant fields.\n    ```python\n    async for event in runner.run_async(...):\n        print(f\"[{event.author}] Event ID: {event.id}, Invocation: {event.invocation_id}\")\n        if event.content and event.content.parts:\n            if event.content.parts[0].text:\n                print(f\"  Text: {event.content.parts[0].text[:100]}...\")\n            if event.get_function_calls():\n                print(f\"  Tool Call: {event.get_function_calls()[0].name} with {event.get_function_calls()[0].args}\")\n            if event.get_function_responses():\n                print(f\"  Tool Response: {event.get_function_responses()[0].response}\")\n        if event.actions:\n            if event.actions.state_delta:\n                print(f\"  State Delta: {event.actions.state_delta}\")\n            if event.actions.transfer_to_agent:\n                print(f\"  TRANSFER TO: {event.actions.transfer_to_agent}\")\n        if event.error_message:\n            print(f\"  ERROR: {event.error_message}\")\n    ```\n*   **Tool/Callback `print` statements**: Simple logging directly within your functions.\n*   **Logging**: Use Python's standard `logging` module. Control verbosity with `adk web --log_level DEBUG` or `adk web -v`.\n*   **Observability Integrations**: ADK supports OpenTelemetry, enabling integration with platforms like:\n    *   Google Cloud Trace\n    *   AgentOps\n    *   Arize AX\n    *   Phoenix\n    *   Weave by WandB\n    ```python\n    # Example using Comet Opik integration (conceptual)\n    # pip install comet_opik_adk\n    # from comet_opik_adk import enable_opik_tracing\n    # enable_opik_tracing() # Call at app startup\n    # Then run your ADK app, traces appear in Comet workspace.\n    ```\n*   **Session History (`session.events`)**: Persisted for detailed post-mortem analysis.\n\n---\n\n## 15. Streaming & Advanced I/O\n\nADK supports real-time, bidirectional communication for interactive experiences like live voice conversations.\n\n*   **Bidirectional Streaming**: Enables low-latency, two-way data flow (text, audio, video) between the client and agent, allowing for interruptions.\n*   **Core Components**:\n    *   **`Runner.run_live()`**: The entry point for starting a streaming session.\n    *   **`LiveRequestQueue`**: A queue for sending data (e.g., audio chunks) from the client to the agent during a live session.\n    *   **`RunConfig`**: A configuration object passed to `run_live()` to specify modalities (`['TEXT', 'AUDIO']`), speech synthesis options, etc.\n*   **Streaming Tools**: A special type of `FunctionTool` that can stream intermediate results back to the agent.\n    *   **Definition**: Must be an `async` function with a return type of `AsyncGenerator`.\n        ```python\n        from typing import AsyncGenerator\n\n        async def monitor_stock_price(symbol: str) -> AsyncGenerator[str, None]:\n            \"\"\"Yields stock price updates as they occur.\"\"\"\n            while True:\n                price = await get_live_price(symbol)\n                yield f\"Update for {symbol}: ${price}\"\n                await asyncio.sleep(5)\n        ```\n\n*   **Advanced I/O Modalities**: ADK (especially with Gemini Live API models) supports richer interactions.\n    *   **Audio**: Input via `Blob(mime_type=\"audio/pcm\", data=bytes)`, Output via `genai_types.SpeechConfig` in `RunConfig`.\n    *   **Vision (Images/Video)**: Input via `Blob(mime_type=\"image/jpeg\", data=bytes)` or `Blob(mime_type=\"video/mp4\", data=bytes)`. Models like `gemini-2.5-flash-exp` can process these.\n    *   **Multimodal Input in `Content`**:\n        ```python\n        multimodal_content = genai_types.Content(\n            parts=[\n                genai_types.Part(text=\"Describe this image:\"),\n                genai_types.Part(inline_data=genai_types.Blob(mime_type=\"image/jpeg\", data=image_bytes))\n            ]\n        )\n        ```\n    *   **Streaming Modalities**: `RunConfig.response_modalities=['TEXT', 'AUDIO']`.\n\n---\n\n## 16. Performance Optimization\n\n*   **Model Selection**: Choose the smallest model that meets requirements (e.g., `gemini-2.5-flash` for simple tasks).\n*   **Instruction Prompt Engineering**: Concise, clear instructions reduce tokens and improve accuracy.\n*   **Tool Use Optimization**:\n    *   Design efficient tools (fast API calls, optimize database queries).\n    *   Cache tool results (e.g., using `before_tool_callback` or `tool_context.state`).\n*   **State Management**: Store only necessary data in state to avoid large context windows.\n*   **`include_contents='none'`**: For stateless utility agents, saves LLM context window.\n*   **Parallelization**: Use `ParallelAgent` for independent tasks.\n*   **Streaming**: Use `StreamingMode.SSE` or `BIDI` for perceived latency reduction.\n*   **`max_llm_calls`**: Limit LLM calls to prevent runaway agents and control costs.\n\n---\n\n## 17. General Best Practices & Common Pitfalls\n\n*   **Start Simple**: Begin with `LlmAgent`, mock tools, and `InMemorySessionService`. Gradually add complexity.\n*   **Iterative Development**: Build small features, test, debug, refine.\n*   **Modular Design**: Use agents and tools to encapsulate logic.\n*   **Clear Naming**: Descriptive names for agents, tools, state keys.\n*   **Error Handling**: Implement robust `try...except` blocks in tools and callbacks. Guide LLMs on how to handle tool errors.\n*   **Testing**: Write unit tests for tools/callbacks, integration tests for agent flows (`pytest`, `adk eval`).\n*   **Dependency Management**: Use virtual environments (`venv`) and `requirements.txt`.\n*   **Secrets Management**: Never hardcode API keys. Use `.env` for local dev, environment variables or secret managers (Google Cloud Secret Manager) for production.\n*   **Avoid Infinite Loops**: Especially with `LoopAgent` or complex LLM tool-calling chains. Use `max_iterations`, `max_llm_calls`, and strong instructions.\n*   **Handle `None` & `Optional`**: Always check for `None` or `Optional` values when accessing nested properties (e.g., `event.content and event.content.parts and event.content.parts[0].text`).\n*   **Immutability of Events**: Events are immutable records. If you need to change something *before* it's processed, do so in a `before_*` callback and return a *new* modified object.\n*   **Understand `output_key` vs. direct `state` writes**: `output_key` is for the agent's *final conversational* output. Direct `tool_context.state['key'] = value` is for *any other* data you want to save.\n*   **Example Agents**: Find practical examples and reference implementations in the [ADK Samples repository](https://github.com/google/adk-samples).\n\n\n### Testing the output of an agent\n\nThe following script demonstrates how to programmatically test an agent's output. This approach is extremely useful when an LLM or coding agent needs to interact with a work-in-progress agent, as well as for automated testing, debugging, or when you need to integrate agent execution into other workflows:\n```\nimport asyncio\n\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom blogger_agent.agent import root_agent\nfrom google.genai import types as genai_types\n\n\nasync def main():\n    \"\"\"Runs the agent with a sample query.\"\"\"\n    session_service = InMemorySessionService()\n    await session_service.create_session(\n        app_name=\"blogger_agent\", user_id=\"test_user\", session_id=\"test_session\"\n    )\n    runner = Runner(\n        agent=root_agent, app_name=\"blogger_agent\", session_service=session_service\n    )\n    query = \"I want a recipe for pancakes\"\n    async for event in runner.run_async(\n        user_id=\"test_user\",\n        session_id=\"test_session\",\n        new_message=genai_types.Content(\n            role=\"user\", \n            parts=[genai_types.Part.from_text(text=query)]\n        ),\n    ):\n        if event.is_final_response():\n            print(event.content.parts[0].text)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n---\n\n## 18. Official API & CLI References\n\nFor detailed specifications of all classes, methods, and commands, refer to the official reference documentation.\n\n*   [Python API Reference](./api-reference/python/index.html)\n*   [Java API Reference](./api-reference/java/index.html)\n*   [CLI Reference](./api-reference/cli/index.html)\n*   [REST API Reference](./api-reference/rest/index.md)\n*   [Agent Config YAML Reference](./api-reference/agentconfig/index.html)\n",
    "llm_txt": "---\n**llm.txt** documents the \"Agent Starter Pack\" repository, providing a source of truth on its purpose, features, and usage.\n---\n\n### Section 1: Project Overview\n\n*   **Project Name:** Agent Starter Pack\n*   **Purpose:** Accelerate development of production-ready GenAI Agents on Google Cloud.\n*   **Tagline:** Production-Ready Agents on Google Cloud, faster.\n\n**The \"Production Gap\":**\nWhile prototyping GenAI agents is quick, production deployment often takes 3-9 months.\n\n**Key Challenges Addressed:**\n*   **Customization:** Business logic, data grounding, security/compliance.\n*   **Evaluation:** Metrics, quality assessment, test datasets.\n*   **Deployment:** Cloud infrastructure, CI/CD, UI integration.\n*   **Observability:** Performance tracking, user feedback.\n\n**Solution: Agent Starter Pack**\nProvides MLOps and infrastructure templates so developers focus on agent logic.\n\n*   **You Build:** Prompts, LLM interactions, business logic, agent orchestration.\n*   **We Provide:**\n    *   Deployment infrastructure, CI/CD, testing\n    *   Logging, monitoring\n    *   Evaluation tools\n    *   Data connections, UI playground\n    *   Security best practices\n\nEstablishes production patterns from day one, saving setup time.\n\n---\n### Section 2: Creating & Enhancing Agent Projects\n\nStart by creating a new agent project from a predefined template, or enhance an existing project with agent capabilities. Both processes support interactive and fully automated setup.\n\n**Prerequisites:**\nBefore you begin, ensure you have `uv`/`uvx`, `gcloud` CLI, `terraform`, `git`, and `gh` CLI (for automated CI/CD setup) installed and authenticated.\n\n**Installing the `agent-starter-pack` CLI:**\nChoose one method to get the `agent-starter-pack` command:\n\n1.  **`uvx` (Recommended for Zero-Install/Automation):** Run directly without prior installation.\n    ```bash\n    uvx agent-starter-pack create ...\n    ```\n2.  **Virtual Environment (`pip` or `uv`):**\n    ```bash\n    pip install agent-starter-pack\n    ```\n3.  **Persistent CLI Install (`pipx` or `uv tool`):** Installs globally in an isolated environment.\n\n---\n### `agent-starter-pack create` Command\n\nGenerates a new agent project directory based on a chosen template and configuration.\n\n**Usage:**\n```bash\nagent-starter-pack create PROJECT_NAME [OPTIONS]\n```\n\n**Arguments:**\n*   `PROJECT_NAME`: Name for your new project directory and base for GCP resource naming (max 26 chars, converted to lowercase).\n\n**Template Selection:**\n*   `-a, --agent`: Agent template - built-in agents (e.g., `adk_base`, `agentic_rag`), remote templates (`adk@gemini-fullstack`, `github.com/user/repo@branch`), or local projects (`local@./path`).\n\n**Deployment Options:**\n*   `-d, --deployment-target`: Target environment (`cloud_run` or `agent_engine`).\n*   `--cicd-runner`: CI/CD runner (`google_cloud_build` or `github_actions`).\n*   `--region`: GCP region (default: `us-central1`).\n\n**Data & Storage:**\n*   `-i, --include-data-ingestion`: Include data ingestion pipeline.\n*   `-ds, --datastore`: Datastore type (`vertex_ai_search`, `vertex_ai_vector_search`, `alloydb`).\n*   `--session-type`: Session storage (`in_memory`, `alloydb`, `agent_engine`).\n\n**Project Creation:**\n*   `-o, --output-dir`: Output directory (default: current directory).\n*   `--agent-directory, -dir`: Agent code directory name (default: `app`).\n*   `--in-folder`: Create files in current directory instead of new subdirectory.\n\n**Automation:**\n*   `--auto-approve`: **Skip all interactive prompts (crucial for automation).**\n*   `--skip-checks`: Skip GCP/Vertex AI verification checks.\n*   `--debug`: Enable debug logging.\n\n**Automated Creation Example:**\n```bash\nuvx agent-starter-pack create my-automated-agent \\\n  -a adk_base \\\n  -d cloud_run \\\n  --region us-central1 \\\n  --auto-approve\n```\n\n---\n\n### `agent-starter-pack enhance` Command\n\nEnhance your existing project with AI agent capabilities by adding agent-starter-pack features in-place. This command supports all the same options as `create` but templates directly into the current directory instead of creating a new project directory.\n\n**Usage:**\n```bash\nagent-starter-pack enhance [TEMPLATE_PATH] [OPTIONS]\n```\n\n**Key Differences from `create`:**\n*   Templates into current directory (equivalent to `create --in-folder`)\n*   `TEMPLATE_PATH` defaults to current directory (`.`)\n*   Project name defaults to current directory name\n*   Additional `--base-template` option to override template inheritance\n\n**Enhanced Project Example:**\n```bash\n# Enhance current directory with agent capabilities\nuvx agent-starter-pack enhance . \\\n  --base-template adk_base \\\n  -d cloud_run \\\n  --region us-central1 \\\n  --auto-approve\n```\n\n**Project Structure:** Expects agent code in `app/` directory (configurable via `--agent-directory`).\n\n---\n\n### Available Agent Templates\n\nTemplates for the `create` command (via `-a` or `--agent`):\n\n| Agent Name             | Description                                  |\n| :--------------------- | :------------------------------------------- |\n| `adk_base`             | Base ReAct agent (ADK)                       |\n| `adk_gemini_fullstack` | Production-ready fullstack research agent    |\n| `agentic_rag`          | RAG agent for document retrieval & Q&A       |\n| `langgraph_base_react` | Base ReAct agent (LangGraph)                 |\n| `crewai_coding_crew`   | Multi-agent collaborative coding assistance  |\n| `live_api`             | Real-time multimodal RAG agent               |\n\n---\n\n### Including a Data Ingestion Pipeline (for RAG agents)\n\nFor RAG agents needing custom document search, enabling this option automates loading, chunking, embedding documents with Vertex AI, and storing them in a vector database.\n\n**How to enable:**\n```bash\nuvx agent-starter-pack create my-rag-agent \\\n  -a agentic_rag \\\n  -d cloud_run \\\n  -i \\\n  -ds vertex_ai_search \\\n  --auto-approve\n```\n**Post-creation:** Follow your new project's `data_ingestion/README.md` to deploy the necessary infrastructure.\n\n---\n### Section 3: Development & Automated Deployment Workflow\n---\n\nThis section describes the end-to-end lifecycle of an agent, with emphasis on automation.\n\n\n### 1. Local Development & Iteration\n\nOnce your project is created, navigate into its directory to begin development.\n\n**First, install dependencies (run once):**\n```bash\nmake install\n```\n\n**Next, test your agent. The recommended method is to use a programmatic script.**\n\n#### Programmatic Testing (Recommended Workflow)\n\nThis method allows for quick, automated validation of your agent's logic.\n\n1.  **Create a script:** In the project's root directory, create a Python script named `run_agent.py`.\n2.  **Invoke the agent:** In the script, write code to programmatically call your agent with sample input and `print()` the output for inspection.\n    *   **Guidance:** If you're unsure or no guidance exists, you can look at files in the `tests/` directory for examples of how to import and call the agent's main function.\n    *   **Important:** This script is for simple validation. **Assertions are not required**, and you should not create a formal `pytest` file.\n3.  **Run the test:** Execute your script from the terminal using `uv`.\n    ```bash\n    uv run python run_agent.py\n    ```\nYou can keep the test file for future testing.\n\n#### Manual Testing with the UI Playground (Optional)\n\nIf the user needs to interact with your agent manually in a chat interface for debugging:\n\n1.  Run the following command to start the local web UI:\n    ```bash\n    make playground\n    ```\n    This is useful for human-in-the-loop testing and features hot-reloading.\n\n### 2. Deploying to a Cloud Development Environment\nBefore setting up full CI/CD, you can deploy to a personal cloud dev environment.\n\n1.  **Set Project:** `gcloud config set project YOUR_DEV_PROJECT_ID`\n2.  **Provision Resources:** `make setup-dev-env` (uses Terraform).\n3.  **Deploy Backend:** `make backend` (builds and deploys the agent).\n\n### 3. Automated Production-Ready Deployment with CI/CD\nFor reliable deployments, the `setup-cicd` command streamlines the entire process. It creates a GitHub repo, connects it to your chosen CI/CD runner (Google Cloud Build or GitHub Actions), provisions staging/prod infrastructure, and configures deployment triggers.\n\n**Automated CI/CD Setup Example (Recommended):**\n```bash\n# Run from the project root. This command will guide you or can be automated with flags.\nuvx agent-starter-pack setup-cicd\n```\n\n**CI/CD Workflow Logic:**\n*   **On Pull Request:** CI pipeline runs tests.\n*   **On Merge to `main`:** CD pipeline deploys to staging.\n*   **Manual Approval:** A manual approval step triggers the production deployment.\n\n---\n### Section 4: Key Features & Customization\n---\n\n### Deploying with a User Interface (UI)\n*   **Unified Deployment (for Dev/Test):** The backend and frontend can be packaged and served from a single Cloud Run service, secured with Identity-Aware Proxy (IAP).\n*   **Deploying with UI:** `make backend IAP=true`\n*   **Access Control:** After deploying with IAP, grant users the `IAP-secured Web App User` role in IAM to give them access.\n\n### Session Management\n\nFor stateful agents, the starter pack supports persistent sessions.\n*   **Cloud Run:** Choose between `in_memory` (for testing) and durable `alloydb` sessions using the `--session-type` flag.\n*   **Agent Engine:** Provides session management automatically.\n\n### Monitoring & Observability\n*   **Technology:** Uses OpenTelemetry to emit events to Google Cloud Trace and Logging.\n*   **Custom Tracer:** A custom tracer in `app/utils/tracing.py` (or a different agent directory instead of app) handles large payloads by linking to GCS, overcoming default service limits.\n*   **Infrastructure:** A Log Router to sink data to BigQuery is provisioned by Terraform.\n\n---\n### Section 5: CLI Reference for CI/CD Setup\n---\n\n### `agent-starter-pack setup-cicd`\nAutomates the complete CI/CD infrastructure setup for GitHub-based deployments. Intelligently detects your CI/CD runner (Google Cloud Build or GitHub Actions) and configures everything automatically.\n\n**Usage:**\n```bash\nuvx agent-starter-pack setup-cicd [OPTIONS]\n```\n\n**Prerequisites:** \n- Run from the project root (directory with `pyproject.toml`)\n- Required tools: `gh` CLI (authenticated), `gcloud` CLI (authenticated), `terraform`\n- `Owner` role on GCP projects\n- GitHub token with `repo` and `workflow` scopes\n\n**Key Options:**\n*   `--staging-project`, `--prod-project`: GCP project IDs (will prompt if omitted).\n*   `--repository-name`, `--repository-owner`: GitHub repo details (will prompt if omitted).\n*   `--cicd-project`: CI/CD resources project (defaults to prod project).\n*   `--dev-project`: Development project ID (optional).\n*   `--region`: GCP region (default: `us-central1`).\n*   `--auto-approve`: Skip all interactive prompts.\n*   `--local-state`: Use local Terraform state instead of GCS backend.\n*   `--debug`: Enable debug logging.\n\n**What it does:**\n1. Creates/connects GitHub repository\n2. Sets up Terraform infrastructure with remote state\n3. Configures CI/CD runner connection (Cloud Build or GitHub Actions with WIF)\n4. Provisions staging/prod environments\n5. Sets up local Git repository with origin remote\n\n**Automated Example:**\n```bash\nuvx agent-starter-pack setup-cicd \\\n  --staging-project your-staging-project \\\n  --prod-project your-prod-project \\\n  --repository-name your-repo-name \\\n  --repository-owner your-username \\\n  --auto-approve\n```\n\n**After setup, push to trigger pipeline:**\n```bash\ngit add . && git commit -m \"Initial commit\" && git push -u origin main\n```\n\n* Note: For coding agents - ask user for required project IDs and repo details before running with `--auto-approve`.\n* Note: If user prefers different git provider, refer to `deployment/README.md` for manual deployment.\n---\n### Section 6: Operational Guidelines for Coding Agents\n\nThese guidelines are essential for interacting with the Agent Starter Pack project effectively.\n\n---\n\n### Principle 1: Code Preservation & Isolation\n\nWhen executing code modifications using tools like `replace` or `write_file`, your paramount objective is surgical precision. You **must alter only the code segments directly targeted** by the user's request, while **strictly preserving all surrounding and unrelated code.**\n\n**Mandatory Pre-Execution Verification:**\n\nBefore finalizing any `new_string` for a `replace` operation, meticulously verify the following:\n\n1.  **Target Identification:** Clearly define the exact lines or expressions to be changed, based *solely* on the user's explicit instructions.\n2.  **Preservation Check:** Compare your proposed `new_string` against the `old_string`. Ensure all code, configuration values (e.g., `model`, `version`, `api_key`), comments, and formatting *outside* the identified target remain identical and verbatim.\n\n**Example: Adhering to Preservation**\n\n*   **User Request:** \"Change the agent's instruction to be a recipe suggester.\"\n*   **Original Code Snippet:**\n    ```python\n    root_agent = Agent(\n        name=\"root_agent\",\n        model=\"gemini-2.5-flash\",\n        instruction=\"You are a helpful AI assistant.\"\n    )\n    ```\n*   **Incorrect Modification (VIOLATION):**\n    ```python\n    root_agent = Agent(\n        name=\"recipe_suggester\",\n        model=\"gemini-1.5-flash\", # UNINTENDED MUTATION - model was not requested to change\n        instruction=\"You are a recipe suggester.\"\n    )\n    ```\n*   **Correct Modification (COMPLIANT):**\n    ```python\n    root_agent = Agent(\n        name=\"recipe_suggester\", # OK, related to new purpose\n        model=\"gemini-2.5-flash\", # MUST be preserved\n        instruction=\"You are a recipe suggester.\" # OK, the direct target\n    )\n    ```\n\n**Critical Error:** Failure to adhere to this preservation principle is a critical error. Always prioritize the integrity of existing, unchanged code over the convenience of rewriting entire blocks.\n\n---\n\n### Principle 2: Workflow & Execution Best Practices\n\n*   **Standard Workflow:**\n    The validated end-to-end process is: `create` \u2192 `test` \u2192 `setup-cicd` \u2192 push to deploy. Trust this high-level workflow as the default for developing and shipping agents.\n\n*   **Agent Testing:**\n    *   **Avoid `make playground`** unless specifically instructed; it is designed for human interaction. Focus on programmatic testing.\n\n*   **Model Selection:**\n    *   **When using Gemini, prefer the 2.5 model family** for optimal performance and capabilities: \"gemini-2.5-pro\" and \"gemini-2.5-flash\"\n\n*   **Running Python Commands:**\n    *   Always use `uv` to execute Python commands within this repository (e.g., `uv run run_agent.py`).\n    *   Ensure project dependencies are installed by running `make install` before executing scripts.\n    *   Consult the project's `Makefile` and `README.md` for other useful development commands.\n\n*   **Further Reading & Troubleshooting:**\n    *   For questions about specific frameworks (e.g., LangGraph) or Google Cloud products (e.g., Cloud Run), their official documentation and online resources are the best source of truth.\n    *   **When encountering persistent errors or if you're unsure how to proceed after initial troubleshooting, a targeted Google Search is strongly recommended.** It is often the fastest way to find relevant documentation, community discussions, or direct solutions to your problem.\n",
    "_copy_without_render": [
      "*.ipynb",
      "*.json",
      "frontend/*",
      "notebooks/*",
      ".git/*",
      "__pycache__/*",
      "**/__pycache__/*",
      ".pytest_cache/*",
      ".venv/*",
      "*templates.py",
      "Makefile",
      "blogger_agent/agent.py"
    ],
    "_template": "/tmp/tmpoadwh7cc/template",
    "_output_dir": "/tmp/tmpoadwh7cc",
    "_repo_dir": "/tmp/tmpoadwh7cc/template",
    "_checkout": null
  },
  "_cookiecutter": {
    "project_name": "myblogagent",
    "agent_name": "remote_2022783209315744818",
    "package_version": "0.14.1",
    "agent_description": "AI-driven agent designed to facilitate the exploration of the blogger agent landscape",
    "example_question": "Write an article about Nano Banana                           ",
    "settings": {
      "requires_data_ingestion": false,
      "requires_session": true,
      "deployment_targets": [
        "agent_engine",
        "cloud_run"
      ],
      "extra_dependencies": [
        "google-adk~=1.8.0"
      ],
      "tags": [
        "adk"
      ],
      "frontend_type": "None",
      "agent_directory": "blogger_agent"
    },
    "tags": [
      "adk"
    ],
    "deployment_target": "cloud_run",
    "cicd_runner": "github_actions",
    "session_type": "in_memory",
    "frontend_type": "None",
    "extra_dependencies": [
      [
        "google-adk~=1.8.0"
      ]
    ],
    "data_ingestion": false,
    "datastore_type": "",
    "agent_directory": "blogger_agent",
    "agent_garden": false,
    "adk_cheatsheet": "# Google Agent Development Kit (ADK) Python Cheatsheet\n\nThis document serves as a long-form, comprehensive reference for building, orchestrating, and deploying AI agents using the Python Agent Development Kit (ADK). It aims to cover every significant aspect with greater detail, more code examples, and in-depth best practices.\n\n## Table of Contents\n\n1.  [Core Concepts & Project Structure](#1-core-concepts--project-structure)\n    *   1.1 ADK's Foundational Principles\n    *   1.2 Essential Primitives\n    *   1.3 Standard Project Layout\n    *   1.A Build Agents without Code (Agent Config)\n2.  [Agent Definitions (`LlmAgent`)](#2-agent-definitions-llmagent)\n    *   2.1 Basic `LlmAgent` Setup\n    *   2.2 Advanced `LlmAgent` Configuration\n    *   2.3 LLM Instruction Crafting\n3.  [Orchestration with Workflow Agents](#3-orchestration-with-workflow-agents)\n    *   3.1 `SequentialAgent`: Linear Execution\n    *   3.2 `ParallelAgent`: Concurrent Execution\n    *   3.3 `LoopAgent`: Iterative Processes\n4.  [Multi-Agent Systems & Communication](#4-multi-agent-systems--communication)\n    *   4.1 Agent Hierarchy\n    *   4.2 Inter-Agent Communication Mechanisms\n    *   4.3 Common Multi-Agent Patterns\n    *   4.A Distributed Communication (A2A Protocol)\n5.  [Building Custom Agents (`BaseAgent`)](#5-building-custom-agents-baseagent)\n    *   5.1 When to Use Custom Agents\n    *   5.2 Implementing `_run_async_impl`\n6.  [Models: Gemini, LiteLLM, and Vertex AI](#6-models-gemini-litellm-and-vertex-ai)\n    *   6.1 Google Gemini Models (AI Studio & Vertex AI)\n    *   6.2 Other Cloud & Proprietary Models via LiteLLM\n    *   6.3 Open & Local Models via LiteLLM (Ollama, vLLM)\n    *   6.4 Customizing LLM API Clients\n7.  [Tools: The Agent's Capabilities](#7-tools-the-agents-capabilities)\n    *   7.1 Defining Function Tools: Principles & Best Practices\n    *   7.2 The `ToolContext` Object: Accessing Runtime Information\n    *   7.3 All Tool Types & Their Usage\n8.  [Context, State, and Memory Management](#8-context-state-and-memory-management)\n    *   8.1 The `Session` Object & `SessionService`\n    *   8.2 `State`: The Conversational Scratchpad\n    *   8.3 `Memory`: Long-Term Knowledge & Retrieval\n    *   8.4 `Artifacts`: Binary Data Management\n9.  [Runtime, Events, and Execution Flow](#9-runtime-events-and-execution-flow)\n    *   9.1 The `Runner`: The Orchestrator\n    *   9.2 The Event Loop: Core Execution Flow\n    *   9.3 `Event` Object: The Communication Backbone\n    *   9.4 Asynchronous Programming (Python Specific)\n10. [Control Flow with Callbacks](#10-control-flow-with-callbacks)\n    *   10.1 Callback Mechanism: Interception & Control\n    *   10.2 Types of Callbacks\n    *   10.3 Callback Best Practices\n    *   10.A Global Control with Plugins\n11. [Authentication for Tools](#11-authentication-for-tools)\n    *   11.1 Core Concepts: `AuthScheme` & `AuthCredential`\n    *   11.2 Interactive OAuth/OIDC Flows\n    *   11.3 Custom Tool Authentication\n12. [Deployment Strategies](#12-deployment-strategies)\n    *   12.1 Local Development & Testing (`adk web`, `adk run`, `adk api_server`)\n    *   12.2 Vertex AI Agent Engine\n    *   12.3 Cloud Run\n    *   12.4 Google Kubernetes Engine (GKE)\n    *   12.5 CI/CD Integration\n13. [Evaluation and Safety](#13-evaluation-and-safety)\n    *   13.1 Agent Evaluation (`adk eval`)\n    *   13.2 Safety & Guardrails\n14. [Debugging, Logging & Observability](#14-debugging-logging--observability)\n15. [Streaming & Advanced I/O](#15-streaming--advanced-io)\n16. [Performance Optimization](#16-performance-optimization)\n17. [General Best Practices & Common Pitfalls](#17-general-best-practices--common-pitfalls)\n18. [Official API & CLI References](#18-official-api--cli-references)\n\n---\n\n## 1. Core Concepts & Project Structure\n\n### 1.1 ADK's Foundational Principles\n\n*   **Modularity**: Break down complex problems into smaller, manageable agents and tools.\n*   **Composability**: Combine simple agents and tools to build sophisticated systems.\n*   **Observability**: Detailed event logging and tracing capabilities to understand agent behavior.\n*   **Extensibility**: Easily integrate with external services, models, and frameworks.\n*   **Deployment-Agnostic**: Design agents once, deploy anywhere.\n\n### 1.2 Essential Primitives\n\n*   **`Agent`**: The core intelligent unit. Can be `LlmAgent` (LLM-driven) or `BaseAgent` (custom/workflow).\n*   **`Tool`**: Callable function/class providing external capabilities (`FunctionTool`, `OpenAPIToolset`, etc.).\n*   **`Session`**: A unique, stateful conversation thread with history (`events`) and short-term memory (`state`).\n*   **`State`**: Key-value dictionary within a `Session` for transient conversation data.\n*   **`Memory`**: Long-term, searchable knowledge base beyond a single session (`MemoryService`).\n*   **`Artifact`**: Named, versioned binary data (files, images) associated with a session or user.\n*   **`Runner`**: The execution engine; orchestrates agent activity and event flow.\n*   **`Event`**: Atomic unit of communication and history; carries content and side-effect `actions`.\n*   **`InvocationContext`**: The comprehensive root context object holding all runtime information for a single `run_async` call.\n\n### 1.3 Standard Project Layout\n\nA well-structured ADK project is crucial for maintainability and leveraging `adk` CLI tools.\n\n```\nyour_project_root/\n\u251c\u2500\u2500 my_first_agent/             # Each folder is a distinct agent app\n\u2502   \u251c\u2500\u2500 __init__.py             # Makes `my_first_agent` a Python package (`from . import agent`)\n\u2502   \u251c\u2500\u2500 agent.py                # Contains `root_agent` definition and `LlmAgent`/WorkflowAgent instances\n\u2502   \u251c\u2500\u2500 tools.py                # Custom tool function definitions\n\u2502   \u251c\u2500\u2500 data/                   # Optional: static data, templates\n\u2502   \u2514\u2500\u2500 .env                    # Environment variables (API keys, project IDs)\n\u251c\u2500\u2500 my_second_agent/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 agent.py\n\u251c\u2500\u2500 requirements.txt            # Project's Python dependencies (e.g., google-adk, litellm)\n\u251c\u2500\u2500 tests/                      # Unit and integration tests\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2502   \u2514\u2500\u2500 test_tools.py\n\u2502   \u2514\u2500\u2500 integration/\n\u2502       \u2514\u2500\u2500 test_my_first_agent.py\n\u2502       \u2514\u2500\u2500 my_first_agent.evalset.json # Evaluation dataset for `adk eval`\n\u2514\u2500\u2500 main.py                     # Optional: Entry point for custom FastAPI server deployment\n```\n*   `adk web` and `adk run` automatically discover agents in subdirectories with `__init__.py` and `agent.py`.\n*   `.env` files are automatically loaded by `adk` tools when run from the root or agent directory.\n\n### 1.A Build Agents without Code (Agent Config)\n\nADK allows you to define agents, tools, and even multi-agent workflows using a simple YAML format, eliminating the need to write Python code for orchestration. This is ideal for rapid prototyping and for non-programmers to configure agents.\n\n#### **Getting Started with Agent Config**\n\n*   **Create a Config-based Agent**:\n    ```bash\n    adk create --type=config my_yaml_agent\n    ```\n    This generates a `my_yaml_agent/` folder with `root_agent.yaml` and `.env` files.\n\n*   **Environment Setup** (in `.env` file):\n    ```bash\n    # For Google AI Studio (simpler setup)\n    GOOGLE_GENAI_USE_VERTEXAI=0\n    GOOGLE_API_KEY=<your-Google-Gemini-API-key>\n    \n    # For Google Cloud Vertex AI (production)\n    GOOGLE_GENAI_USE_VERTEXAI=1\n    GOOGLE_CLOUD_PROJECT=<your_gcp_project>\n    GOOGLE_CLOUD_LOCATION=us-central1\n    ```\n\n#### **Core Agent Config Structure**\n\n*   **Basic Agent (`root_agent.yaml`)**:\n    ```yaml\n    # yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json\n    name: assistant_agent\n    model: gemini-2.5-flash\n    description: A helper agent that can answer users' various questions.\n    instruction: You are an agent to help answer users' various questions.\n    ```\n\n*   **Agent with Built-in Tools**:\n    ```yaml\n    name: search_agent\n    model: gemini-2.0-flash\n    description: 'an agent whose job it is to perform Google search queries and answer questions about the results.'\n    instruction: You are an agent whose job is to perform Google search queries and answer questions about the results.\n    tools:\n      - name: google_search # Built-in ADK tool\n    ```\n\n*   **Agent with Custom Tools**:\n    ```yaml\n    agent_class: LlmAgent\n    model: gemini-2.5-flash\n    name: prime_agent\n    description: Handles checking if numbers are prime.\n    instruction: |\n      You are responsible for checking whether numbers are prime.\n      When asked to check primes, you must call the check_prime tool with a list of integers.\n      Never attempt to determine prime numbers manually.\n    tools:\n      - name: ma_llm.check_prime # Reference to Python function\n    ```\n\n*   **Multi-Agent System with Sub-Agents**:\n    ```yaml\n    agent_class: LlmAgent\n    model: gemini-2.5-flash\n    name: root_agent\n    description: Learning assistant that provides tutoring in code and math.\n    instruction: |\n      You are a learning assistant that helps students with coding and math questions.\n      \n      You delegate coding questions to the code_tutor_agent and math questions to the math_tutor_agent.\n      \n      Follow these steps:\n      1. If the user asks about programming or coding, delegate to the code_tutor_agent.\n      2. If the user asks about math concepts or problems, delegate to the math_tutor_agent.\n      3. Always provide clear explanations and encourage learning.\n    sub_agents:\n      - config_path: code_tutor_agent.yaml\n      - config_path: math_tutor_agent.yaml\n    ```\n\n#### **Loading Agent Config in Python**\n\n```python\nfrom google.adk.agents import config_agent_utils\nroot_agent = config_agent_utils.from_config(\"{agent_folder}/root_agent.yaml\")\n```\n\n#### **Running Agent Config Agents**\n\nFrom the agent directory, use any of these commands:\n*   `adk web` - Launch web UI interface\n*   `adk run` - Run in terminal without UI\n*   `adk api_server` - Run as a service for other applications\n\n#### **Deployment Support**\n\nAgent Config agents can be deployed using:\n*   `adk deploy cloud_run` - Deploy to Google Cloud Run\n*   `adk deploy agent_engine` - Deploy to Vertex AI Agent Engine\n\n#### **Key Features & Capabilities**\n\n*   **Supported Built-in Tools**: `google_search`, `load_artifacts`, `url_context`, `exit_loop`, `preload_memory`, `get_user_choice`, `enterprise_web_search`, `load_web_page`\n*   **Custom Tool Integration**: Reference Python functions using fully qualified module paths\n*   **Multi-Agent Orchestration**: Link agents via `config_path` references\n*   **Schema Validation**: Built-in YAML schema for IDE support and validation\n\n#### **Current Limitations** (Experimental Feature)\n\n*   **Model Support**: Only Gemini models currently supported\n*   **Language Support**: Custom tools must be written in Python\n*   **Unsupported Agent Types**: `LangGraphAgent`, `A2aAgent`\n*   **Unsupported Tools**: `AgentTool`, `LongRunningFunctionTool`, `VertexAiSearchTool`, `MCPToolset`, `CrewaiTool`, `LangchainTool`, `ExampleTool`\n\nFor complete examples and reference, see the [ADK samples repository](https://github.com/search?q=repo%3Agoogle%2Fadk-python+path%3A%2F%5Econtributing%5C%2Fsamples%5C%2F%2F+.yaml&type=code).\n\n---\n\n## 2. Agent Definitions (`LlmAgent`)\n\nThe `LlmAgent` is the cornerstone of intelligent behavior, leveraging an LLM for reasoning and decision-making.\n\n### 2.1 Basic `LlmAgent` Setup\n\n```python\nfrom google.adk.agents import Agent\n\ndef get_current_time(city: str) -> dict:\n    \"\"\"Returns the current time in a specified city.\"\"\"\n    # Mock implementation\n    if city.lower() == \"new york\":\n        return {\"status\": \"success\", \"time\": \"10:30 AM EST\"}\n    return {\"status\": \"error\", \"message\": f\"Time for {city} not available.\"}\n\nmy_first_llm_agent = Agent(\n    name=\"time_teller_agent\",\n    model=\"gemini-2.5-flash\", # Essential: The LLM powering the agent\n    instruction=\"You are a helpful assistant that tells the current time in cities. Use the 'get_current_time' tool for this purpose.\",\n    description=\"Tells the current time in a specified city.\", # Crucial for multi-agent delegation\n    tools=[get_current_time] # List of callable functions/tool instances\n)\n```\n\n### 2.2 Advanced `LlmAgent` Configuration\n\n*   **`generate_content_config`**: Controls LLM generation parameters (temperature, token limits, safety).\n    ```python\n    from google.genai import types as genai_types\n    from google.adk.agents import Agent\n\n    gen_config = genai_types.GenerateContentConfig(\n        temperature=0.2,            # Controls randomness (0.0-1.0), lower for more deterministic.\n        top_p=0.9,                  # Nucleus sampling: sample from top_p probability mass.\n        top_k=40,                   # Top-k sampling: sample from top_k most likely tokens.\n        max_output_tokens=1024,     # Max tokens in LLM's response.\n        stop_sequences=[\"## END\"]   # LLM will stop generating if these sequences appear.\n    )\n    agent = Agent(\n        # ... basic config ...\n        generate_content_config=gen_config\n    )\n    ```\n\n*   **`output_key`**: Automatically saves the agent's final text or structured (if `output_schema` is used) response to the `session.state` under this key. Facilitates data flow between agents.\n    ```python\n    agent = Agent(\n        # ... basic config ...\n        output_key=\"llm_final_response_text\"\n    )\n    # After agent runs, session.state['llm_final_response_text'] will contain its output.\n    ```\n\n*   **`input_schema` & `output_schema`**: Define strict JSON input/output formats using Pydantic models.\n    > **Warning**: Using `output_schema` forces the LLM to generate JSON and **disables** its ability to use tools or delegate to other agents.\n\n#### **Example: Defining and Using Structured Output**\n\nThis is the most reliable way to make an LLM produce predictable, parseable JSON, which is essential for multi-agent workflows.\n\n1.  **Define the Schema with Pydantic:**\n    ```python\n    from pydantic import BaseModel, Field\n    from typing import Literal\n\n    class SearchQuery(BaseModel):\n        \"\"\"Model representing a specific search query for web search.\"\"\"\n        search_query: str = Field(\n            description=\"A highly specific and targeted query for web search.\"\n        )\n\n    class Feedback(BaseModel):\n        \"\"\"Model for providing evaluation feedback on research quality.\"\"\"\n        grade: Literal[\"pass\", \"fail\"] = Field(\n            description=\"Evaluation result. 'pass' if the research is sufficient, 'fail' if it needs revision.\"\n        )\n        comment: str = Field(\n            description=\"Detailed explanation of the evaluation, highlighting strengths and/or weaknesses of the research.\"\n        )\n        follow_up_queries: list[SearchQuery] | None = Field(\n            default=None,\n            description=\"A list of specific, targeted follow-up search queries needed to fix research gaps. This should be null or empty if the grade is 'pass'.\"\n        )\n    ```\n    *   **`BaseModel` & `Field`**: Define data types, defaults, and crucial `description` fields. These descriptions are sent to the LLM to guide its output.\n    *   **`Literal`**: Enforces strict enum-like values (`\"pass\"` or `\"fail\"`), preventing the LLM from hallucinating unexpected values.\n\n2.  **Assign the Schema to an `LlmAgent`:**\n    ```python\n    research_evaluator = LlmAgent(\n        name=\"research_evaluator\",\n        model=\"gemini-2.5-pro\",\n        instruction=\"\"\"You are a meticulous quality assurance analyst. Evaluate the research findings in 'section_research_findings' and be very critical.\n        If you find significant gaps, assign a grade of 'fail', write a detailed comment, and generate 5-7 specific follow-up queries.\n        If the research is thorough, grade it 'pass'.\n        Your response must be a single, raw JSON object validating against the 'Feedback' schema.\n        \"\"\",\n        output_schema=Feedback, # This forces the LLM to output JSON matching the Feedback model.\n        output_key=\"research_evaluation\", # The resulting JSON object will be saved to state.\n        disallow_transfer_to_peers=True, # Prevents this agent from delegating. Its job is only to evaluate.\n    )\n    ```\n\n*   **`include_contents`**: Controls whether the conversation history is sent to the LLM.\n    *   `'default'` (default): Sends relevant history.\n    *   `'none'`: Sends no history; agent operates purely on current turn's input and `instruction`. Useful for stateless API wrapper agents.\n    ```python\n    agent = Agent(..., include_contents='none')\n    ```\n\n*   **`planner`**: Assign a `BasePlanner` instance to enable multi-step reasoning.\n    *   **`BuiltInPlanner`**: Leverages a model's native \"thinking\" or planning capabilities (e.g., Gemini).\n        ```python\n        from google.adk.planners import BuiltInPlanner\n        from google.genai.types import ThinkingConfig\n\n        agent = Agent(\n            model=\"gemini-2.5-flash\",\n            planner=BuiltInPlanner(\n                thinking_config=ThinkingConfig(include_thoughts=True)\n            ),\n            # ... tools ...\n        )\n        ```\n    *   **`PlanReActPlanner`**: Instructs the model to follow a structured Plan-Reason-Act output format, useful for models without built-in planning.\n\n*   **`code_executor`**: Assign a `BaseCodeExecutor` to allow the agent to execute code blocks.\n    *   **`BuiltInCodeExecutor`**: The standard, sandboxed code executor provided by ADK for safe execution.\n        ```python\n        from google.adk.code_executors import BuiltInCodeExecutor\n        agent = Agent(\n            name=\"code_agent\",\n            model=\"gemini-2.5-flash\",\n            instruction=\"Write and execute Python code to solve math problems.\",\n            code_executor=BuiltInCodeExecutor() # Corrected from a list to an instance\n        )\n        ```\n\n*   **Callbacks**: Hooks for observing and modifying agent behavior at key lifecycle points (`before_model_callback`, `after_tool_callback`, etc.). (Covered in Callbacks).\n\n### 2.3 LLM Instruction Crafting (`instruction`)\n\nThe `instruction` is critical. It guides the LLM's behavior, persona, and tool usage. The following examples demonstrate powerful techniques for creating specialized, reliable agents.\n\n**Best Practices & Examples:**\n\n*   **Be Specific & Concise**: Avoid ambiguity.\n*   **Define Persona & Role**: Give the LLM a clear role.\n*   **Constrain Behavior & Tool Use**: Explicitly state what the LLM should *and should not* do.\n*   **Define Output Format**: Tell the LLM *exactly* what its output should look like, especially when not using `output_schema`.\n*   **Dynamic Injection**: Use `{state_key}` to inject runtime data from `session.state` into the prompt.\n*   **Iteration**: Test, observe, and refine instructions.\n\n**Example 1: Constraining Tool Use and Output Format**\n```python\nimport datetime\nfrom google.adk.tools import google_search   \n\n\nplan_generator = LlmAgent(\n    model=\"gemini-2.5-flash\",\n    name=\"plan_generator\",\n    description=\"Generates a 4-5 line action-oriented research plan.\",\n    instruction=f\"\"\"\n    You are a research strategist. Your job is to create a high-level RESEARCH PLAN, not a summary.\n    **RULE: Your output MUST be a bulleted list of 4-5 action-oriented research goals or key questions.**\n    - A good goal starts with a verb like \"Analyze,\" \"Identify,\" \"Investigate.\"\n    - A bad output is a statement of fact like \"The event was in April 2024.\"\n    **TOOL USE IS STRICTLY LIMITED:**\n    Your goal is to create a generic, high-quality plan *without searching*.\n    Only use `google_search` if a topic is ambiguous and you absolutely cannot create a plan without it.\n    You are explicitly forbidden from researching the *content* or *themes* of the topic.\n    Current date: {datetime.datetime.now().strftime(\"%Y-%m-%d\")}\n    \"\"\",\n    tools=[google_search],\n)\n```\n\n**Example 2: Injecting Data from State and Specifying Custom Tags**\nThis agent's `instruction` relies on data placed in `session.state` by previous agents.\n```python\nreport_composer = LlmAgent(\n    model=\"gemini-2.5-pro\",\n    name=\"report_composer_with_citations\",\n    include_contents=\"none\", # History not needed; all data is injected.\n    description=\"Transforms research data and a markdown outline into a final, cited report.\",\n    instruction=\"\"\"\n    Transform the provided data into a polished, professional, and meticulously cited research report.\n\n    ---\n    ### INPUT DATA\n    *   Research Plan: `{research_plan}`\n    *   Research Findings: `{section_research_findings}`\n    *   Citation Sources: `{sources}`\n    *   Report Structure: `{report_sections}`\n\n    ---\n    ### CRITICAL: Citation System\n    To cite a source, you MUST insert a special citation tag directly after the claim it supports.\n\n    **The only correct format is:** `<cite source=\"src-ID_NUMBER\" />`\n\n    ---\n    ### Final Instructions\n    Generate a comprehensive report using ONLY the `<cite source=\"src-ID_NUMBER\" />` tag system for all citations.\n    The final report must strictly follow the structure provided in the **Report Structure** markdown outline.\n    Do not include a \"References\" or \"Sources\" section; all citations must be in-line.\n    \"\"\",\n    output_key=\"final_cited_report\",\n)\n```\n\n---\n\n## 3. Orchestration with Workflow Agents\n\nWorkflow agents (`SequentialAgent`, `ParallelAgent`, `LoopAgent`) provide deterministic control flow, combining LLM capabilities with structured execution. They do **not** use an LLM for their own orchestration logic.\n\n### 3.1 `SequentialAgent`: Linear Execution\n\nExecutes `sub_agents` one after another in the order defined. The `InvocationContext` is passed along, allowing state changes to be visible to subsequent agents.\n\n```python\nfrom google.adk.agents import SequentialAgent, Agent\n\n# Agent 1: Summarizes a document and saves to state\nsummarizer = Agent(\n    name=\"DocumentSummarizer\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"Summarize the provided document in 3 sentences.\",\n    output_key=\"document_summary\" # Output saved to session.state['document_summary']\n)\n\n# Agent 2: Generates questions based on the summary from state\nquestion_generator = Agent(\n    name=\"QuestionGenerator\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"Generate 3 comprehension questions based on this summary: {document_summary}\",\n    # 'document_summary' is dynamically injected from session.state\n)\n\ndocument_pipeline = SequentialAgent(\n    name=\"SummaryQuestionPipeline\",\n    sub_agents=[summarizer, question_generator], # Order matters!\n    description=\"Summarizes a document then generates questions.\"\n)\n```\n\n### 3.2 `ParallelAgent`: Concurrent Execution\n\nExecutes `sub_agents` simultaneously. Useful for independent tasks to reduce overall latency. All sub-agents share the same `session.state`.\n\n```python\nfrom google.adk.agents import ParallelAgent, Agent, SequentialAgent\n\n# Agents to fetch data concurrently\nfetch_stock_price = Agent(name=\"StockPriceFetcher\", ..., output_key=\"stock_data\")\nfetch_news_headlines = Agent(name=\"NewsFetcher\", ..., output_key=\"news_data\")\nfetch_social_sentiment = Agent(name=\"SentimentAnalyzer\", ..., output_key=\"sentiment_data\")\n\n# Agent to merge results (runs after ParallelAgent, usually in a SequentialAgent)\nmerger_agent = Agent(\n    name=\"ReportGenerator\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"Combine stock data: {stock_data}, news: {news_data}, and sentiment: {sentiment_data} into a market report.\"\n)\n\n# Pipeline to run parallel fetching then sequential merging\nmarket_analysis_pipeline = SequentialAgent(\n    name=\"MarketAnalyzer\",\n    sub_agents=[\n        ParallelAgent(\n            name=\"ConcurrentFetch\",\n            sub_agents=[fetch_stock_price, fetch_news_headlines, fetch_social_sentiment]\n        ),\n        merger_agent # Runs after all parallel agents complete\n    ]\n)\n```\n*   **Concurrency Caution**: When parallel agents write to the same `state` key, race conditions can occur. Always use distinct `output_key`s or manage concurrent writes explicitly.\n\n### 3.3 `LoopAgent`: Iterative Processes\n\nRepeatedly executes its `sub_agents` (sequentially within each loop iteration) until a condition is met or `max_iterations` is reached.\n\n#### **Termination of `LoopAgent`**\nA `LoopAgent` terminates when:\n1.  `max_iterations` is reached.\n2.  Any `Event` yielded by a sub-agent (or a tool within it) sets `actions.escalate = True`. This provides dynamic, content-driven loop termination.\n\n#### **Example: Iterative Refinement Loop with a Custom `BaseAgent` for Control**\nThis example shows a loop that continues until a condition, determined by an evaluation agent, is met.\n\n```python\nfrom google.adk.agents import LoopAgent, Agent, BaseAgent\nfrom google.adk.events import Event, EventActions\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom typing import AsyncGenerator\n\n# An LLM Agent that evaluates research and produces structured JSON output\nresearch_evaluator = Agent(\n    name=\"research_evaluator\",\n    # ... configuration from Section 2.2 ...\n    output_schema=Feedback,\n    output_key=\"research_evaluation\",\n)\n\n# An LLM Agent that performs additional searches based on feedback\nenhanced_search_executor = Agent(\n    name=\"enhanced_search_executor\",\n    instruction=\"Execute the follow-up queries from 'research_evaluation' and combine with existing findings.\",\n    # ... other configurations ...\n)\n\n# A custom BaseAgent to check the evaluation and stop the loop\nclass EscalationChecker(BaseAgent):\n    \"\"\"Checks research evaluation and escalates to stop the loop if grade is 'pass'.\"\"\"\n    async def _run_async_impl(self, ctx: InvocationContext) -> AsyncGenerator[Event, None]:\n        evaluation = ctx.session.state.get(\"research_evaluation\")\n        if evaluation and evaluation.get(\"grade\") == \"pass\":\n            # The key to stopping the loop: yield an Event with escalate=True\n            yield Event(author=self.name, actions=EventActions(escalate=True))\n        else:\n            # Let the loop continue\n            yield Event(author=self.name)\n\n# Define the loop\niterative_refinement_loop = LoopAgent(\n    name=\"IterativeRefinementLoop\",\n    sub_agents=[\n        research_evaluator, # Step 1: Evaluate\n        EscalationChecker(name=\"EscalationChecker\"), # Step 2: Check and maybe stop\n        enhanced_search_executor, # Step 3: Refine (only runs if loop didn't stop)\n    ],\n    max_iterations=5, # Fallback to prevent infinite loops\n    description=\"Iteratively evaluates and refines research until it passes quality checks.\"\n)\n```\n\n---\n\n## 4. Multi-Agent Systems & Communication\n\nBuilding complex applications by composing multiple, specialized agents.\n\n### 4.1 Agent Hierarchy\n\nA hierarchical (tree-like) structure of parent-child relationships defined by the `sub_agents` parameter during `BaseAgent` initialization. An agent can only have one parent.\n\n```python\n# Conceptual Hierarchy\n# Root\n# \u2514\u2500\u2500 Coordinator (LlmAgent)\n#     \u251c\u2500\u2500 SalesAgent (LlmAgent)\n#     \u2514\u2500\u2500 SupportAgent (LlmAgent)\n#     \u2514\u2500\u2500 DataPipeline (SequentialAgent)\n#         \u251c\u2500\u2500 DataFetcher (LlmAgent)\n#         \u2514\u2500\u2500 DataProcessor (LlmAgent)\n```\n\n### 4.2 Inter-Agent Communication Mechanisms\n\n1.  **Shared Session State (`session.state`)**: The most common and robust method. Agents read from and write to the same mutable dictionary.\n    *   **Mechanism**: Agent A sets `ctx.session.state['key'] = value`. Agent B later reads `ctx.session.state.get('key')`. `output_key` on `LlmAgent` is a convenient auto-setter.\n    *   **Best for**: Passing intermediate results, shared configurations, and flags in pipelines (Sequential, Loop agents).\n\n2.  **LLM-Driven Delegation (`transfer_to_agent`)**: A `LlmAgent` can dynamically hand over control to another agent based on its reasoning.\n    *   **Mechanism**: The LLM generates a special `transfer_to_agent` function call. The ADK framework intercepts this, routes the next turn to the target agent.\n    *   **Prerequisites**:\n        *   The initiating `LlmAgent` needs `instruction` to guide delegation and `description` of the target agent(s).\n        *   Target agents need clear `description`s to help the LLM decide.\n        *   Target agent must be discoverable within the current agent's hierarchy (direct `sub_agent` or a descendant).\n    *   **Configuration**: Can be enabled/disabled via `disallow_transfer_to_parent` and `disallow_transfer_to_peers` on `LlmAgent`.\n\n3.  **Explicit Invocation (`AgentTool`)**: An `LlmAgent` can treat another `BaseAgent` instance as a callable tool.\n    *   **Mechanism**: Wrap the target agent (`target_agent`) in `AgentTool(agent=target_agent)` and add it to the calling `LlmAgent`'s `tools` list. The `AgentTool` generates a `FunctionDeclaration` for the LLM. When called, `AgentTool` runs the target agent and returns its final response as the tool result.\n    *   **Best for**: Hierarchical task decomposition, where a higher-level agent needs a specific output from a lower-level agent.\n\n### 4.3 Common Multi-Agent Patterns\n\n*   **Coordinator/Dispatcher**: A central agent routes requests to specialized sub-agents (often via LLM-driven delegation).\n*   **Sequential Pipeline**: `SequentialAgent` orchestrates a fixed sequence of tasks, passing data via shared state.\n*   **Parallel Fan-Out/Gather**: `ParallelAgent` runs concurrent tasks, followed by a final agent that synthesizes results from state.\n*   **Review/Critique (Generator-Critic)**: `SequentialAgent` with a generator followed by a critic, often in a `LoopAgent` for iterative refinement.\n*   **Hierarchical Task Decomposition (Planner/Executor)**: High-level agents break down complex problems, delegating sub-tasks to lower-level agents (often via `AgentTool` and delegation).\n\n#### **Example: Hierarchical Planner/Executor Pattern**\nThis pattern combines several mechanisms. A top-level `interactive_planner_agent` uses another agent (`plan_generator`) as a tool to create a plan, then delegates the execution of that plan to a complex `SequentialAgent` (`research_pipeline`).\n\n```python\nfrom google.adk.agents import LlmAgent, SequentialAgent, LoopAgent\nfrom google.adk.tools.agent_tool import AgentTool\n\n# Assume plan_generator, section_planner, research_evaluator, etc. are defined.\n\n# The execution pipeline itself is a complex agent.\nresearch_pipeline = SequentialAgent(\n    name=\"research_pipeline\",\n    description=\"Executes a pre-approved research plan. It performs iterative research, evaluation, and composes a final, cited report.\",\n    sub_agents=[\n        section_planner,\n        section_researcher,\n        LoopAgent(\n            name=\"iterative_refinement_loop\",\n            max_iterations=3,\n            sub_agents=[\n                research_evaluator,\n                EscalationChecker(name=\"escalation_checker\"),\n                enhanced_search_executor,\n            ],\n        ),\n        report_composer,\n    ],\n)\n\n# The top-level agent that interacts with the user.\ninteractive_planner_agent = LlmAgent(\n    name=\"interactive_planner_agent\",\n    model=\"gemini-2.5-flash\",\n    description=\"The primary research assistant. It collaborates with the user to create a research plan, and then executes it upon approval.\",\n    instruction=\"\"\"\n    You are a research planning assistant. Your workflow is:\n    1.  **Plan:** Use the `plan_generator` tool to create a draft research plan.\n    2.  **Refine:** Incorporate user feedback until the plan is approved.\n    3.  **Execute:** Once the user gives EXPLICIT approval (e.g., \"looks good, run it\"), you MUST delegate the task to the `research_pipeline` agent.\n    Your job is to Plan, Refine, and Delegate. Do not do the research yourself.\n    \"\"\",\n    # The planner delegates to the pipeline.\n    sub_agents=[research_pipeline],\n    # The planner uses another agent as a tool.\n    tools=[AgentTool(plan_generator)],\n    output_key=\"research_plan\",\n)\n\n# The root agent of the application is the top-level planner.\nroot_agent = interactive_planner_agent\n```\n\n### 4.A. Distributed Communication (A2A Protocol)\n\nThe Agent-to-Agent (A2A) Protocol enables agents to communicate over a network, even if they are written in different languages or run as separate services. Use A2A for integrating with third-party agents, building microservice-based agent architectures, or when a strong, formal API contract is needed. For internal code organization, prefer local sub-agents.\n\n*   **Exposing an Agent**: Make an existing ADK agent available to others over A2A.\n    *   **`to_a2a()` Utility**: The simplest method. Wraps your `root_agent` and creates a runnable FastAPI app, auto-generating the required `agent.json` card.\n        ```python\n        from google.adk.a2a.utils.agent_to_a2a import to_a2a\n        # root_agent is your existing ADK Agent instance\n        a2a_app = to_a2a(root_agent, port=8001)\n        # Run with: uvicorn your_module:a2a_app --host localhost --port 8001\n        ```\n    *   **`adk api_server --a2a`**: A CLI command that serves agents from a directory. Requires you to manually create an `agent.json` card for each agent you want to expose.\n\n*   **Consuming a Remote Agent**: Use a remote A2A agent as if it were a local agent.\n    *   **`RemoteA2aAgent`**: This agent acts as a client proxy. You initialize it with the URL to the remote agent's card.\n        ```python\n        from google.adk.a2a.remote_a2a_agent import RemoteA2aAgent\n\n        # This agent can now be used as a sub-agent or tool\n        prime_checker_agent = RemoteA2aAgent(\n            name=\"prime_agent\",\n            description=\"A remote agent that checks if numbers are prime.\",\n            agent_card=\"http://localhost:8001/a2a/check_prime_agent/.well-known/agent.json\"\n        )\n        ```\n\n---\n\n## 5. Building Custom Agents (`BaseAgent`)\n\nFor unique orchestration logic that doesn't fit standard workflow agents, inherit directly from `BaseAgent`.\n\n### 5.1 When to Use Custom Agents\n\n*   **Complex Conditional Logic**: `if/else` branching based on multiple state variables.\n*   **Dynamic Agent Selection**: Choosing which sub-agent to run based on runtime evaluation.\n*   **Direct External Integrations**: Calling external APIs or libraries directly within the orchestration flow.\n*   **Custom Loop/Retry Logic**: More sophisticated iteration patterns than `LoopAgent`, such as the `EscalationChecker` example.\n\n### 5.2 Implementing `_run_async_impl`\n\nThis is the core asynchronous method you must override.\n\n#### **Example: A Custom Agent for Loop Control**\nThis agent reads state, applies simple Python logic, and yields an `Event` with an `escalate` action to control a `LoopAgent`.\n\n```python\nfrom google.adk.agents import BaseAgent\nfrom google.adk.agents.invocation_context import InvocationContext\nfrom google.adk.events import Event, EventActions\nfrom typing import AsyncGenerator\nimport logging\n\nclass EscalationChecker(BaseAgent):\n    \"\"\"Checks research evaluation and escalates to stop the loop if grade is 'pass'.\"\"\"\n\n    def __init__(self, name: str):\n        super().__init__(name=name)\n\n    async def _run_async_impl(\n        self, ctx: InvocationContext\n    ) -> AsyncGenerator[Event, None]:\n        # 1. Read from session state.\n        evaluation_result = ctx.session.state.get(\"research_evaluation\")\n\n        # 2. Apply custom Python logic.\n        if evaluation_result and evaluation_result.get(\"grade\") == \"pass\":\n            logging.info(\n                f\"[{self.name}] Research passed. Escalating to stop loop.\"\n            )\n            # 3. Yield an Event with a control Action.\n            yield Event(author=self.name, actions=EventActions(escalate=True))\n        else:\n            logging.info(\n                f\"[{self.name}] Research failed or not found. Loop continues.\"\n            )\n            # Yielding an event without actions lets the flow continue.\n            yield Event(author=self.name)\n```\n*   **Asynchronous Generator**: `async def ... yield Event`. This allows pausing and resuming execution.\n*   **`ctx: InvocationContext`**: Provides access to all session state (`ctx.session.state`).\n*   **Calling Sub-Agents**: Use `async for event in self.sub_agent_instance.run_async(ctx): yield event`.\n*   **Control Flow**: Use standard Python `if/else`, `for/while` loops for complex logic.\n\n---\n\n## 6. Models: Gemini, LiteLLM, and Vertex AI\n\nADK's model flexibility allows integrating various LLMs for different needs.\n\n### 6.1 Google Gemini Models (AI Studio & Vertex AI)\n\n*   **Default Integration**: Native support via `google-genai` library.\n*   **AI Studio (Easy Start)**:\n    *   Set `GOOGLE_API_KEY=\"YOUR_API_KEY\"` (environment variable).\n    *   Set `GOOGLE_GENAI_USE_VERTEXAI=\"False\"`.\n    *   Model strings: `\"gemini-2.5-flash\"`, `\"gemini-2.5-pro\"`, etc.\n*   **Vertex AI (Production)**:\n    *   Authenticate via `gcloud auth application-default login` (recommended).\n    *   Set `GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\"`, `GOOGLE_CLOUD_LOCATION=\"your-region\"` (environment variables).\n    *   Set `GOOGLE_GENAI_USE_VERTEXAI=\"True\"`.\n    *   Model strings: `\"gemini-2.5-flash\"`, `\"gemini-2.5-pro\"`, or full Vertex AI endpoint resource names for specific deployments.\n\n### 6.2 Other Cloud & Proprietary Models via LiteLLM\n\n`LiteLlm` provides a unified interface to 100+ LLMs (OpenAI, Anthropic, Cohere, etc.).\n\n*   **Installation**: `pip install litellm`\n*   **API Keys**: Set environment variables as required by LiteLLM (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`).\n*   **Usage**:\n    ```python\n    from google.adk.models.lite_llm import LiteLlm\n    agent_openai = Agent(model=LiteLlm(model=\"openai/gpt-4o\"), ...)\n    agent_claude = Agent(model=LiteLlm(model=\"anthropic/claude-3-haiku-20240307\"), ...)\n    ```\n\n### 6.3 Open & Local Models via LiteLLM (Ollama, vLLM)\n\nFor self-hosting, cost savings, privacy, or offline use.\n\n*   **Ollama Integration**: Run Ollama locally (`ollama run <model>`).\n    ```bash\n    export OLLAMA_API_BASE=\"http://localhost:11434\" # Ensure Ollama server is running\n    ```\n    ```python\n    from google.adk.models.lite_llm import LiteLlm\n    # Use 'ollama_chat' provider for tool-calling capabilities with Ollama models\n    agent_ollama = Agent(model=LiteLlm(model=\"ollama_chat/llama3:instruct\"), ...)\n    ```\n\n*   **Self-Hosted Endpoint (e.g., vLLM)**:\n    ```python\n    from google.adk.models.lite_llm import LiteLlm\n    api_base_url = \"https://your-vllm-endpoint.example.com/v1\"\n    agent_vllm = Agent(\n        model=LiteLlm(\n            model=\"your-model-name-on-vllm\",\n            api_base=api_base_url,\n            extra_headers={\"Authorization\": \"Bearer YOUR_TOKEN\"},\n        ),\n        ...\n    )\n    ```\n\n### 6.4 Customizing LLM API Clients\n\nFor `google-genai` (used by Gemini models), you can configure the underlying client.\n\n```python\nimport os\nfrom google.genai import configure as genai_configure\n\ngenai_configure.use_defaults(\n    timeout=60, # seconds\n    client_options={\"api_key\": os.getenv(\"GOOGLE_API_KEY\")},\n)\n```\n\n---\n\n## 7. Tools: The Agent's Capabilities\n\nTools extend an agent's abilities beyond text generation.\n\n### 7.1 Defining Function Tools: Principles & Best Practices\n\n*   **Signature**: `def my_tool(param1: Type, param2: Type, tool_context: ToolContext) -> dict:`\n*   **Function Name**: Descriptive verb-noun (e.g., `schedule_meeting`).\n*   **Parameters**: Clear names, required type hints, **NO DEFAULT VALUES**.\n*   **Return Type**: **Must** be a `dict` (JSON-serializable), preferably with a `'status'` key.\n*   **Docstring**: **CRITICAL**. Explain purpose, when to use, arguments, and return value structure. **AVOID** mentioning `tool_context`.\n\n    ```python\n    def calculate_compound_interest(\n        principal: float,\n        rate: float,\n        years: int,\n        compounding_frequency: int,\n        tool_context: ToolContext\n    ) -> dict:\n        \"\"\"Calculates the future value of an investment with compound interest.\n\n        Use this tool to calculate the future value of an investment given a\n        principal amount, interest rate, number of years, and how often the\n        interest is compounded per year.\n\n        Args:\n            principal (float): The initial amount of money invested.\n            rate (float): The annual interest rate (e.g., 0.05 for 5%).\n            years (int): The number of years the money is invested.\n            compounding_frequency (int): The number of times interest is compounded\n                                         per year (e.g., 1 for annually, 12 for monthly).\n            \n        Returns:\n            dict: Contains the calculation result.\n                  - 'status' (str): \"success\" or \"error\".\n                  - 'future_value' (float, optional): The calculated future value.\n                  - 'error_message' (str, optional): Description of error, if any.\n        \"\"\"\n        # ... implementation ...\n    ```\n\n### 7.2 The `ToolContext` Object: Accessing Runtime Information\n\n`ToolContext` is the gateway for tools to interact with the ADK runtime.\n\n*   `tool_context.state`: Read and write to the current `Session`'s `state` dictionary.\n*   `tool_context.actions`: Modify the `EventActions` object (e.g., `tool_context.actions.escalate = True`).\n*   `tool_context.load_artifact(filename)` / `tool_context.save_artifact(filename, part)`: Manage binary data.\n*   `tool_context.search_memory(query)`: Query the long-term `MemoryService`.\n\n### 7.3 All Tool Types & Their Usage\n\n1.  **Custom Function Tools**:\n    *   **`FunctionTool`**: The most common type, wrapping a standard Python function.\n    *   **`LongRunningFunctionTool`**: Wraps an `async` function that `yields` intermediate results, for tasks that provide progress updates.\n    *   **`AgentTool`**: Wraps another `BaseAgent` instance, allowing it to be invoked as a tool by a parent agent.\n\n2.  **Built-in Tools**: Ready-to-use tools provided by ADK.\n    *   `google_search`: Provides Google Search grounding.\n    *   `BuiltInCodeExecutor`: Enables sandboxed code execution.\n    *   `VertexAiSearchTool`: Provides grounding from your private Vertex AI Search data stores.\n    *   `BigQueryToolset`: A collection of tools for interacting with BigQuery (e.g., `list_datasets`, `execute_sql`).\n    > **Warning**: An agent can only use one type of built-in tool at a time and they cannot be used in sub-agents.\n\n3.  **Third-Party Tool Wrappers**: For seamless integration with other frameworks.\n    *   `LangchainTool`: Wraps a tool from the LangChain ecosystem.\n    *   `CrewaiTool`: Wraps a tool from the CrewAI library.\n\n4.  **OpenAPI & Protocol Tools**: For interacting with APIs and services.\n    *   **`OpenAPIToolset`**: Automatically generates a set of `RestApiTool`s from an OpenAPI (Swagger) v3 specification.\n    *   **`MCPToolset`**: Connects to an external Model Context Protocol (MCP) server to dynamically load its tools.\n\n5.  **Google Cloud Tools**: For deep integration with Google Cloud services.\n    *   **`ApiHubToolset`**: Turns any documented API from Apigee API Hub into a tool.\n    *   **`ApplicationIntegrationToolset`**: Turns Application Integration workflows and Integration Connectors (e.g., Salesforce, SAP) into callable tools.\n    *   **Toolbox for Databases**: An open-source MCP server that ADK can connect to for database interactions.\n\n---\n\n## 8. Context, State, and Memory Management\n\nEffective context management is crucial for coherent, multi-turn conversations.\n\n### 8.1 The `Session` Object & `SessionService`\n\n*   **`Session`**: The container for a single, ongoing conversation (`id`, `state`, `events`).\n*   **`SessionService`**: Manages the lifecycle of `Session` objects (`create_session`, `get_session`, `append_event`).\n*   **Implementations**: `InMemorySessionService` (dev), `VertexAiSessionService` (prod), `DatabaseSessionService` (self-managed).\n\n### 8.2 `State`: The Conversational Scratchpad\n\nA mutable dictionary within `session.state` for short-term, dynamic data.\n\n*   **Update Mechanism**: Always update via `context.state` (in callbacks/tools) or `LlmAgent.output_key`.\n*   **Prefixes for Scope**:\n    *   **(No prefix)**: Session-specific (e.g., `session.state['booking_step']`).\n    *   `user:`: Persistent for a `user_id` across all their sessions (e.g., `session.state['user:preferred_currency']`).\n    *   `app:`: Persistent for `app_name` across all users and sessions.\n    *   `temp:`: Volatile, for the current `Invocation` turn only.\n\n### 8.3 `Memory`: Long-Term Knowledge & Retrieval\n\nFor knowledge beyond a single conversation.\n\n*   **`BaseMemoryService`**: Defines the interface (`add_session_to_memory`, `search_memory`).\n*   **Implementations**: `InMemoryMemoryService`, `VertexAiRagMemoryService`.\n*   **Usage**: Agents interact via tools (e.g., the built-in `load_memory` tool).\n\n### 8.4 `Artifacts`: Binary Data Management\n\nFor named, versioned binary data (files, images).\n\n*   **Representation**: `google.genai.types.Part` (containing a `Blob` with `data: bytes` and `mime_type: str`).\n*   **`BaseArtifactService`**: Manages storage (`save_artifact`, `load_artifact`).\n*   **Implementations**: `InMemoryArtifactService`, `GcsArtifactService`.\n\n---\n\n## 9. Runtime, Events, and Execution Flow\n\nThe `Runner` is the central orchestrator of an ADK application.\n\n### 9.1 The `Runner`: The Orchestrator\n\n*   **Role**: Manages the agent's lifecycle, the event loop, and coordinates with services.\n*   **Entry Point**: `runner.run_async(user_id, session_id, new_message)`.\n\n### 9.2 The Event Loop: Core Execution Flow\n\n1.  User input becomes a `user` `Event`.\n2.  `Runner` calls `agent.run_async(invocation_context)`.\n3.  Agent `yield`s an `Event` (e.g., tool call, text response). Execution pauses.\n4.  `Runner` processes the `Event` (applies state changes, etc.) and yields it to the client.\n5.  Execution resumes. This cycle repeats until the agent is done.\n\n### 9.3 `Event` Object: The Communication Backbone\n\n`Event` objects carry all information and signals.\n\n*   `Event.author`: Source of the event (`'user'`, agent name, `'system'`).\n*   `Event.content`: The primary payload (text, function calls, function responses).\n*   `Event.actions`: Signals side effects (`state_delta`, `transfer_to_agent`, `escalate`).\n*   `Event.is_final_response()`: Helper to identify the complete, displayable message.\n\n### 9.4 Asynchronous Programming (Python Specific)\n\nADK is built on `asyncio`. Use `async def`, `await`, and `async for` for all I/O-bound operations.\n\n---\n\n## 10. Control Flow with Callbacks\n\nCallbacks are functions that intercept and control agent execution at specific points.\n\n### 10.1 Callback Mechanism: Interception & Control\n\n*   **Definition**: A Python function assigned to an agent's `callback` parameter (e.g., `after_agent_callback=my_func`).\n*   **Context**: Receives a `CallbackContext` (or `ToolContext`) with runtime info.\n*   **Return Value**: **Crucially determines flow.**\n    *   `return None`: Allow the default action to proceed.\n    *   `return <Specific Object>`: **Override** the default action/result.\n\n### 10.2 Types of Callbacks\n\n1.  **Agent Lifecycle**: `before_agent_callback`, `after_agent_callback`.\n2.  **LLM Interaction**: `before_model_callback`, `after_model_callback`.\n3.  **Tool Execution**: `before_tool_callback`, `after_tool_callback`.\n\n### 10.3 Callback Best Practices\n\n*   **Keep Focused**: Each callback for a single purpose.\n*   **Performance**: Avoid blocking I/O or heavy computation.\n*   **Error Handling**: Use `try...except` to prevent crashes.\n\n#### **Example 1: Data Aggregation with `after_agent_callback`**\nThis callback runs after an agent, inspects the `session.events` to find structured data from tool calls (like `google_search` results), and saves it to state for later use.\n\n```python\nfrom google.adk.agents.callback_context import CallbackContext\n\ndef collect_research_sources_callback(callback_context: CallbackContext) -> None:\n    \"\"\"Collects and organizes web research sources from agent events.\"\"\"\n    session = callback_context._invocation_context.session\n    # Get existing sources from state to append to them.\n    url_to_short_id = callback_context.state.get(\"url_to_short_id\", {})\n    sources = callback_context.state.get(\"sources\", {})\n    id_counter = len(url_to_short_id) + 1\n\n    # Iterate through all events in the session to find grounding metadata.\n    for event in session.events:\n        if not (event.grounding_metadata and event.grounding_metadata.grounding_chunks):\n            continue\n        # ... logic to parse grounding_chunks and grounding_supports ...\n        # (See full implementation in the original code snippet)\n\n    # Save the updated source map back to state.\n    callback_context.state[\"url_to_short_id\"] = url_to_short_id\n    callback_context.state[\"sources\"] = sources\n\n# Used in an agent like this:\n# section_researcher = LlmAgent(..., after_agent_callback=collect_research_sources_callback)\n```\n\n#### **Example 2: Output Transformation with `after_agent_callback`**\nThis callback takes an LLM's raw output (containing custom tags), uses Python to format it into markdown, and returns the modified content, overriding the original.\n\n```python\nimport re\nfrom google.adk.agents.callback_context import CallbackContext\nfrom google.genai import types as genai_types\n\ndef citation_replacement_callback(callback_context: CallbackContext) -> genai_types.Content:\n    \"\"\"Replaces <cite> tags in a report with Markdown-formatted links.\"\"\"\n    # 1. Get raw report and sources from state.\n    final_report = callback_context.state.get(\"final_cited_report\", \"\")\n    sources = callback_context.state.get(\"sources\", {})\n\n    # 2. Define a replacer function for regex substitution.\n    def tag_replacer(match: re.Match) -> str:\n        short_id = match.group(1)\n        if not (source_info := sources.get(short_id)):\n            return \"\" # Remove invalid tags\n        title = source_info.get(\"title\", short_id)\n        return f\" [{title}]({source_info['url']})\"\n\n    # 3. Use regex to find all <cite> tags and replace them.\n    processed_report = re.sub(\n        r'<cite\\s+source\\s*=\\s*[\"\\']?(src-\\d+)[\"\\']?\\s*/>',\n        tag_replacer,\n        final_report,\n    )\n    processed_report = re.sub(r\"\\s+([.,;:])\", r\"\\1\", processed_report) # Fix spacing\n\n    # 4. Save the new version to state and return it to override the original agent output.\n    callback_context.state[\"final_report_with_citations\"] = processed_report\n    return genai_types.Content(parts=[genai_types.Part(text=processed_report)])\n\n# Used in an agent like this:\n# report_composer = LlmAgent(..., after_agent_callback=citation_replacement_callback)\n```\n\n### 10.A. Global Control with Plugins\n\nPlugins are stateful, reusable modules for implementing cross-cutting concerns that apply globally to all agents, tools, and model calls managed by a `Runner`. Unlike Callbacks which are configured per-agent, Plugins are registered once on the `Runner`.\n\n*   **Use Cases**: Ideal for universal logging, application-wide policy enforcement, global caching, and collecting metrics.\n*   **Execution Order**: Plugin callbacks run **before** their corresponding agent-level callbacks. If a plugin callback returns a value, the agent-level callback is skipped.\n*   **Defining a Plugin**: Inherit from `BasePlugin` and implement callback methods.\n    ```python\n    from google.adk.plugins.base_plugin import BasePlugin\n    from google.adk.agents.callback_context import CallbackContext\n\n    class InvocationCounterPlugin(BasePlugin):\n        def __init__(self):\n            super().__init__(name=\"invocation_counter\")\n            self.agent_runs = 0\n\n        async def before_agent_callback(self, callback_context: CallbackContext, **kwargs):\n            self.agent_runs += 1\n            print(f\"[Plugin] Total agent runs: {self.agent_runs}\")\n    ```\n*   **Registering a Plugin**:\n    ```python\n    from google.adk.runners import Runner\n    # runner = Runner(agent=root_agent, ..., plugins=[InvocationCounterPlugin()])\n    ```\n*   **Error Handling Callbacks**: Plugins support unique error hooks like `on_model_error_callback` and `on_tool_error_callback` for centralized error management.\n*   **Limitation**: Plugins are not supported by the `adk web` interface.\n\n---\n\n## 11. Authentication for Tools\n\nEnabling agents to securely access protected external resources.\n\n### 11.1 Core Concepts: `AuthScheme` & `AuthCredential`\n\n*   **`AuthScheme`**: Defines *how* an API expects authentication (e.g., `APIKey`, `HTTPBearer`, `OAuth2`, `OpenIdConnectWithConfig`).\n*   **`AuthCredential`**: Holds *initial* information to *start* the auth process (e.g., API key value, OAuth client ID/secret).\n\n### 11.2 Interactive OAuth/OIDC Flows\n\nWhen a tool requires user interaction (OAuth consent), ADK pauses and signals your `Agent Client` application.\n\n1.  **Detect Auth Request**: `runner.run_async()` yields an event with a special `adk_request_credential` function call.\n2.  **Redirect User**: Extract `auth_uri` from `auth_config` in the event. Your client app redirects the user's browser to this `auth_uri` (appending `redirect_uri`).\n3.  **Handle Callback**: Your client app has a pre-registered `redirect_uri` to receive the user after authorization. It captures the full callback URL (containing `authorization_code`).\n4.  **Send Auth Result to ADK**: Your client prepares a `FunctionResponse` for `adk_request_credential`, setting `auth_config.exchanged_auth_credential.oauth2.auth_response_uri` to the captured callback URL.\n5.  **Resume Execution**: `runner.run_async()` is called again with this `FunctionResponse`. ADK performs the token exchange, stores the access token, and retries the original tool call.\n\n### 11.3 Custom Tool Authentication\n\nIf building a `FunctionTool` that needs authentication:\n\n1.  **Check for Cached Creds**: `tool_context.state.get(\"my_token_cache_key\")`.\n2.  **Check for Auth Response**: `tool_context.get_auth_response(my_auth_config)`.\n3.  **Initiate Auth**: If no creds, call `tool_context.request_credential(my_auth_config)` and return a pending status. This triggers the external flow.\n4.  **Cache Credentials**: After obtaining, store in `tool_context.state`.\n5.  **Make API Call**: Use the valid credentials (e.g., `google.oauth2.credentials.Credentials`).\n\n---\n\n## 12. Deployment Strategies\n\nFrom local dev to production.\n\n### 12.1 Local Development & Testing (`adk web`, `adk run`, `adk api_server`)\n\n*   **`adk web`**: Launches a local web UI for interactive chat, session inspection, and visual tracing.\n    ```bash\n    adk web /path/to/your/project_root\n    ```\n*   **`adk run`**: Command-line interactive chat.\n    ```bash\n    adk run /path/to/your/agent_folder\n    ```\n*   **`adk api_server`**: Launches a local FastAPI server exposing `/run`, `/run_sse`, `/list-apps`, etc., for API testing with `curl` or client libraries.\n    ```bash\n    adk api_server /path/to/your/project_root\n    ```\n\n### 12.2 Vertex AI Agent Engine\n\nFully managed, scalable service for ADK agents on Google Cloud.\n\n*   **Features**: Auto-scaling, session management, observability integration.\n*   **ADK CLI**: `adk deploy agent_engine --project <id> --region <loc> ... /path/to/agent`\n*   **Deployment**: Use `vertexai.agent_engines.create()`.\n    ```python\n    from vertexai.preview import reasoning_engines # or agent_engines directly in later versions\n    \n    # Wrap your root_agent for deployment\n    app_for_engine = reasoning_engines.AdkApp(agent=root_agent, enable_tracing=True)\n    \n    # Deploy\n    remote_app = agent_engines.create(\n        agent_engine=app_for_engine,\n        requirements=[\"google-cloud-aiplatform[adk,agent_engines]\"],\n        display_name=\"My Production Agent\"\n    )\n    print(remote_app.resource_name) # projects/PROJECT_NUM/locations/REGION/reasoningEngines/ID\n    ```\n*   **Interaction**: Use `remote_app.stream_query()`, `create_session()`, etc.\n\n### 12.3 Cloud Run\n\nServerless container platform for custom web applications.\n\n*   **ADK CLI**: `adk deploy cloud_run --project <id> --region <loc> ... /path/to/agent`\n*   **Deployment**:\n    1.  Create a `Dockerfile` for your FastAPI app (using `google.adk.cli.fast_api.get_fast_api_app`).\n    2.  Use `gcloud run deploy --source .`.\n    3.  Alternatively, `adk deploy cloud_run` (simpler, opinionated).\n*   **Example `main.py`**:\n    ```python\n    import os\n    from fastapi import FastAPI\n    from google.adk.cli.fast_api import get_fast_api_app\n\n    # Ensure your agent_folder (e.g., 'my_first_agent') is in the same directory as main.py\n    app: FastAPI = get_fast_api_app(\n        agents_dir=os.path.dirname(os.path.abspath(__file__)),\n        session_service_uri=\"sqlite:///./sessions.db\", # In-container SQLite, for simple cases\n        # For production: use a persistent DB (Cloud SQL) or VertexAiSessionService\n        allow_origins=[\"*\"],\n        web=True # Serve ADK UI\n    )\n    # uvicorn.run(app, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080))) # If running directly\n    ```\n\n### 12.4 Google Kubernetes Engine (GKE)\n\nFor maximum control, run your containerized agent in a Kubernetes cluster.\n\n*   **ADK CLI**: `adk deploy gke --project <id> --cluster_name <name> ... /path/to/agent`\n*   **Deployment**:\n    1.  Build Docker image (`gcloud builds submit`).\n    2.  Create Kubernetes Deployment and Service YAMLs.\n    3.  Apply with `kubectl apply -f deployment.yaml`.\n    4.  Configure Workload Identity for GCP permissions.\n\n### 12.5 CI/CD Integration\n\n*   Automate testing (`pytest`, `adk eval`) in CI.\n*   Automate container builds and deployments (e.g., Cloud Build, GitHub Actions).\n*   Use environment variables for secrets.\n\n---\n\n## 13. Evaluation and Safety\n\nCritical for robust, production-ready agents.\n\n### 13.1 Agent Evaluation (`adk eval`)\n\nSystematically assess agent performance using predefined test cases.\n\n*   **Evalset File (`.evalset.json`)**: Contains `eval_cases`, each with a `conversation` (user queries, expected tool calls, expected intermediate/final responses) and `session_input` (initial state).\n    ```json\n    {\n      \"eval_set_id\": \"weather_bot_eval\",\n      \"eval_cases\": [\n        {\n          \"eval_id\": \"london_weather_query\",\n          \"conversation\": [\n            {\n              \"user_content\": {\"parts\": [{\"text\": \"What's the weather in London?\"}]},\n              \"final_response\": {\"parts\": [{\"text\": \"The weather in London is cloudy...\"}]},\n              \"intermediate_data\": {\n                \"tool_uses\": [{\"name\": \"get_weather\", \"args\": {\"city\": \"London\"}}]\n              }\n            }\n          ],\n          \"session_input\": {\"app_name\": \"weather_app\", \"user_id\": \"test_user\", \"state\": {}}\n        }\n      ]\n    }\n    ```\n*   **Running Evaluation**:\n    *   `adk web`: Interactive UI for creating/running eval cases.\n    *   `adk eval /path/to/agent_folder /path/to/evalset.json`: CLI execution.\n    *   `pytest`: Integrate `AgentEvaluator.evaluate()` into unit/integration tests.\n*   **Metrics**: `tool_trajectory_avg_score` (tool calls match expected), `response_match_score` (final response similarity using ROUGE). Configurable via `test_config.json`.\n\n### 13.2 Safety & Guardrails\n\nMulti-layered defense against harmful content, misalignment, and unsafe actions.\n\n1.  **Identity and Authorization**:\n    *   **Agent-Auth**: Tool acts with the agent's service account (e.g., `Vertex AI User` role). Simple, but all users share access level. Logs needed for attribution.\n    *   **User-Auth**: Tool acts with the end-user's identity (via OAuth tokens). Reduces risk of abuse.\n2.  **In-Tool Guardrails**: Design tools defensively. Tools can read policies from `tool_context.state` (set deterministically by developer) and validate model-provided arguments before execution.\n    ```python\n    def execute_sql(query: str, tool_context: ToolContext) -> dict:\n        policy = tool_context.state.get(\"user:sql_policy\", {})\n        if not policy.get(\"allow_writes\", False) and (\"INSERT\" in query.upper() or \"DELETE\" in query.upper()):\n            return {\"status\": \"error\", \"message\": \"Policy: Write operations are not allowed.\"}\n        # ... execute query ...\n    ```\n3.  **Built-in Gemini Safety Features**:\n    *   **Content Safety Filters**: Automatically block harmful content (CSAM, PII, hate speech, etc.). Configurable thresholds.\n    *   **System Instructions**: Guide model behavior, define prohibited topics, brand tone, disclaimers.\n4.  **Model and Tool Callbacks (LLM as a Guardrail)**: Use callbacks to inspect inputs/outputs.\n    *   `before_model_callback`: Intercept `LlmRequest` before it hits the LLM. Block (return `LlmResponse`) or modify.\n    *   `before_tool_callback`: Intercept tool calls (name, args) before execution. Block (return `dict`) or modify.\n    *   **LLM-based Safety**: Use a cheap/fast LLM (e.g., Gemini Flash) in a callback to classify input/output safety.\n        ```python\n        def safety_checker_callback(context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n            # Use a separate, small LLM to classify safety\n            safety_llm_agent = Agent(name=\"SafetyChecker\", model=\"gemini-2.5-flash-001\", instruction=\"Classify input as 'safe' or 'unsafe'. Output ONLY the word.\")\n            # Run the safety agent (might need a new runner instance or direct model call)\n            # For simplicity, a mock:\n            user_input = llm_request.contents[-1].parts[0].text\n            if \"dangerous_phrase\" in user_input.lower():\n                context.state[\"safety_violation\"] = True\n                return LlmResponse(content=genai_types.Content(parts=[genai_types.Part(text=\"I cannot process this request due to safety concerns.\")]))\n            return None\n        ```\n5.  **Sandboxed Code Execution**:\n    *   `BuiltInCodeExecutor`: Uses secure, sandboxed execution environments.\n    *   Vertex AI Code Interpreter Extension.\n    *   If custom, ensure hermetic environments (no network, isolated).\n6.  **Network Controls & VPC-SC**: Confine agent activity within secure perimeters (VPC Service Controls) to prevent data exfiltration.\n7.  **Output Escaping in UIs**: Always properly escape LLM-generated content in web UIs to prevent XSS attacks and indirect prompt injections.\n\n**Grounding**: A key safety and reliability feature that connects agent responses to verifiable information.\n*   **Mechanism**: Uses tools like `google_search` or `VertexAiSearchTool` to fetch real-time or private data.\n*   **Benefit**: Reduces model hallucination by basing responses on retrieved facts.\n*   **Requirement**: When using `google_search`, your application UI **must** display the provided search suggestions and citations to comply with terms of service.\n\n---\n\n## 14. Debugging, Logging & Observability\n\n*   **`adk web` UI**: Best first step. Provides visual trace, session history, and state inspection.\n*   **Event Stream Logging**: Iterate `runner.run_async()` events and print relevant fields.\n    ```python\n    async for event in runner.run_async(...):\n        print(f\"[{event.author}] Event ID: {event.id}, Invocation: {event.invocation_id}\")\n        if event.content and event.content.parts:\n            if event.content.parts[0].text:\n                print(f\"  Text: {event.content.parts[0].text[:100]}...\")\n            if event.get_function_calls():\n                print(f\"  Tool Call: {event.get_function_calls()[0].name} with {event.get_function_calls()[0].args}\")\n            if event.get_function_responses():\n                print(f\"  Tool Response: {event.get_function_responses()[0].response}\")\n        if event.actions:\n            if event.actions.state_delta:\n                print(f\"  State Delta: {event.actions.state_delta}\")\n            if event.actions.transfer_to_agent:\n                print(f\"  TRANSFER TO: {event.actions.transfer_to_agent}\")\n        if event.error_message:\n            print(f\"  ERROR: {event.error_message}\")\n    ```\n*   **Tool/Callback `print` statements**: Simple logging directly within your functions.\n*   **Logging**: Use Python's standard `logging` module. Control verbosity with `adk web --log_level DEBUG` or `adk web -v`.\n*   **Observability Integrations**: ADK supports OpenTelemetry, enabling integration with platforms like:\n    *   Google Cloud Trace\n    *   AgentOps\n    *   Arize AX\n    *   Phoenix\n    *   Weave by WandB\n    ```python\n    # Example using Comet Opik integration (conceptual)\n    # pip install comet_opik_adk\n    # from comet_opik_adk import enable_opik_tracing\n    # enable_opik_tracing() # Call at app startup\n    # Then run your ADK app, traces appear in Comet workspace.\n    ```\n*   **Session History (`session.events`)**: Persisted for detailed post-mortem analysis.\n\n---\n\n## 15. Streaming & Advanced I/O\n\nADK supports real-time, bidirectional communication for interactive experiences like live voice conversations.\n\n*   **Bidirectional Streaming**: Enables low-latency, two-way data flow (text, audio, video) between the client and agent, allowing for interruptions.\n*   **Core Components**:\n    *   **`Runner.run_live()`**: The entry point for starting a streaming session.\n    *   **`LiveRequestQueue`**: A queue for sending data (e.g., audio chunks) from the client to the agent during a live session.\n    *   **`RunConfig`**: A configuration object passed to `run_live()` to specify modalities (`['TEXT', 'AUDIO']`), speech synthesis options, etc.\n*   **Streaming Tools**: A special type of `FunctionTool` that can stream intermediate results back to the agent.\n    *   **Definition**: Must be an `async` function with a return type of `AsyncGenerator`.\n        ```python\n        from typing import AsyncGenerator\n\n        async def monitor_stock_price(symbol: str) -> AsyncGenerator[str, None]:\n            \"\"\"Yields stock price updates as they occur.\"\"\"\n            while True:\n                price = await get_live_price(symbol)\n                yield f\"Update for {symbol}: ${price}\"\n                await asyncio.sleep(5)\n        ```\n\n*   **Advanced I/O Modalities**: ADK (especially with Gemini Live API models) supports richer interactions.\n    *   **Audio**: Input via `Blob(mime_type=\"audio/pcm\", data=bytes)`, Output via `genai_types.SpeechConfig` in `RunConfig`.\n    *   **Vision (Images/Video)**: Input via `Blob(mime_type=\"image/jpeg\", data=bytes)` or `Blob(mime_type=\"video/mp4\", data=bytes)`. Models like `gemini-2.5-flash-exp` can process these.\n    *   **Multimodal Input in `Content`**:\n        ```python\n        multimodal_content = genai_types.Content(\n            parts=[\n                genai_types.Part(text=\"Describe this image:\"),\n                genai_types.Part(inline_data=genai_types.Blob(mime_type=\"image/jpeg\", data=image_bytes))\n            ]\n        )\n        ```\n    *   **Streaming Modalities**: `RunConfig.response_modalities=['TEXT', 'AUDIO']`.\n\n---\n\n## 16. Performance Optimization\n\n*   **Model Selection**: Choose the smallest model that meets requirements (e.g., `gemini-2.5-flash` for simple tasks).\n*   **Instruction Prompt Engineering**: Concise, clear instructions reduce tokens and improve accuracy.\n*   **Tool Use Optimization**:\n    *   Design efficient tools (fast API calls, optimize database queries).\n    *   Cache tool results (e.g., using `before_tool_callback` or `tool_context.state`).\n*   **State Management**: Store only necessary data in state to avoid large context windows.\n*   **`include_contents='none'`**: For stateless utility agents, saves LLM context window.\n*   **Parallelization**: Use `ParallelAgent` for independent tasks.\n*   **Streaming**: Use `StreamingMode.SSE` or `BIDI` for perceived latency reduction.\n*   **`max_llm_calls`**: Limit LLM calls to prevent runaway agents and control costs.\n\n---\n\n## 17. General Best Practices & Common Pitfalls\n\n*   **Start Simple**: Begin with `LlmAgent`, mock tools, and `InMemorySessionService`. Gradually add complexity.\n*   **Iterative Development**: Build small features, test, debug, refine.\n*   **Modular Design**: Use agents and tools to encapsulate logic.\n*   **Clear Naming**: Descriptive names for agents, tools, state keys.\n*   **Error Handling**: Implement robust `try...except` blocks in tools and callbacks. Guide LLMs on how to handle tool errors.\n*   **Testing**: Write unit tests for tools/callbacks, integration tests for agent flows (`pytest`, `adk eval`).\n*   **Dependency Management**: Use virtual environments (`venv`) and `requirements.txt`.\n*   **Secrets Management**: Never hardcode API keys. Use `.env` for local dev, environment variables or secret managers (Google Cloud Secret Manager) for production.\n*   **Avoid Infinite Loops**: Especially with `LoopAgent` or complex LLM tool-calling chains. Use `max_iterations`, `max_llm_calls`, and strong instructions.\n*   **Handle `None` & `Optional`**: Always check for `None` or `Optional` values when accessing nested properties (e.g., `event.content and event.content.parts and event.content.parts[0].text`).\n*   **Immutability of Events**: Events are immutable records. If you need to change something *before* it's processed, do so in a `before_*` callback and return a *new* modified object.\n*   **Understand `output_key` vs. direct `state` writes**: `output_key` is for the agent's *final conversational* output. Direct `tool_context.state['key'] = value` is for *any other* data you want to save.\n*   **Example Agents**: Find practical examples and reference implementations in the [ADK Samples repository](https://github.com/google/adk-samples).\n\n\n### Testing the output of an agent\n\nThe following script demonstrates how to programmatically test an agent's output. This approach is extremely useful when an LLM or coding agent needs to interact with a work-in-progress agent, as well as for automated testing, debugging, or when you need to integrate agent execution into other workflows:\n```\nimport asyncio\n\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom {{cookiecutter.agent_directory}}.agent import root_agent\nfrom google.genai import types as genai_types\n\n\nasync def main():\n    \"\"\"Runs the agent with a sample query.\"\"\"\n    session_service = InMemorySessionService()\n    await session_service.create_session(\n        app_name=\"{{cookiecutter.agent_directory}}\", user_id=\"test_user\", session_id=\"test_session\"\n    )\n    runner = Runner(\n        agent=root_agent, app_name=\"{{cookiecutter.agent_directory}}\", session_service=session_service\n    )\n    query = \"I want a recipe for pancakes\"\n    async for event in runner.run_async(\n        user_id=\"test_user\",\n        session_id=\"test_session\",\n        new_message=genai_types.Content(\n            role=\"user\", \n            parts=[genai_types.Part.from_text(text=query)]\n        ),\n    ):\n        if event.is_final_response():\n            print(event.content.parts[0].text)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n---\n\n## 18. Official API & CLI References\n\nFor detailed specifications of all classes, methods, and commands, refer to the official reference documentation.\n\n*   [Python API Reference](./api-reference/python/index.html)\n*   [Java API Reference](./api-reference/java/index.html)\n*   [CLI Reference](./api-reference/cli/index.html)\n*   [REST API Reference](./api-reference/rest/index.md)\n*   [Agent Config YAML Reference](./api-reference/agentconfig/index.html)\n",
    "llm_txt": "---\n**llm.txt** documents the \"Agent Starter Pack\" repository, providing a source of truth on its purpose, features, and usage.\n---\n\n### Section 1: Project Overview\n\n*   **Project Name:** Agent Starter Pack\n*   **Purpose:** Accelerate development of production-ready GenAI Agents on Google Cloud.\n*   **Tagline:** Production-Ready Agents on Google Cloud, faster.\n\n**The \"Production Gap\":**\nWhile prototyping GenAI agents is quick, production deployment often takes 3-9 months.\n\n**Key Challenges Addressed:**\n*   **Customization:** Business logic, data grounding, security/compliance.\n*   **Evaluation:** Metrics, quality assessment, test datasets.\n*   **Deployment:** Cloud infrastructure, CI/CD, UI integration.\n*   **Observability:** Performance tracking, user feedback.\n\n**Solution: Agent Starter Pack**\nProvides MLOps and infrastructure templates so developers focus on agent logic.\n\n*   **You Build:** Prompts, LLM interactions, business logic, agent orchestration.\n*   **We Provide:**\n    *   Deployment infrastructure, CI/CD, testing\n    *   Logging, monitoring\n    *   Evaluation tools\n    *   Data connections, UI playground\n    *   Security best practices\n\nEstablishes production patterns from day one, saving setup time.\n\n---\n### Section 2: Creating & Enhancing Agent Projects\n\nStart by creating a new agent project from a predefined template, or enhance an existing project with agent capabilities. Both processes support interactive and fully automated setup.\n\n**Prerequisites:**\nBefore you begin, ensure you have `uv`/`uvx`, `gcloud` CLI, `terraform`, `git`, and `gh` CLI (for automated CI/CD setup) installed and authenticated.\n\n**Installing the `agent-starter-pack` CLI:**\nChoose one method to get the `agent-starter-pack` command:\n\n1.  **`uvx` (Recommended for Zero-Install/Automation):** Run directly without prior installation.\n    ```bash\n    uvx agent-starter-pack create ...\n    ```\n2.  **Virtual Environment (`pip` or `uv`):**\n    ```bash\n    pip install agent-starter-pack\n    ```\n3.  **Persistent CLI Install (`pipx` or `uv tool`):** Installs globally in an isolated environment.\n\n---\n### `agent-starter-pack create` Command\n\nGenerates a new agent project directory based on a chosen template and configuration.\n\n**Usage:**\n```bash\nagent-starter-pack create PROJECT_NAME [OPTIONS]\n```\n\n**Arguments:**\n*   `PROJECT_NAME`: Name for your new project directory and base for GCP resource naming (max 26 chars, converted to lowercase).\n\n**Template Selection:**\n*   `-a, --agent`: Agent template - built-in agents (e.g., `adk_base`, `agentic_rag`), remote templates (`adk@gemini-fullstack`, `github.com/user/repo@branch`), or local projects (`local@./path`).\n\n**Deployment Options:**\n*   `-d, --deployment-target`: Target environment (`cloud_run` or `agent_engine`).\n*   `--cicd-runner`: CI/CD runner (`google_cloud_build` or `github_actions`).\n*   `--region`: GCP region (default: `us-central1`).\n\n**Data & Storage:**\n*   `-i, --include-data-ingestion`: Include data ingestion pipeline.\n*   `-ds, --datastore`: Datastore type (`vertex_ai_search`, `vertex_ai_vector_search`, `alloydb`).\n*   `--session-type`: Session storage (`in_memory`, `alloydb`, `agent_engine`).\n\n**Project Creation:**\n*   `-o, --output-dir`: Output directory (default: current directory).\n*   `--agent-directory, -dir`: Agent code directory name (default: `app`).\n*   `--in-folder`: Create files in current directory instead of new subdirectory.\n\n**Automation:**\n*   `--auto-approve`: **Skip all interactive prompts (crucial for automation).**\n*   `--skip-checks`: Skip GCP/Vertex AI verification checks.\n*   `--debug`: Enable debug logging.\n\n**Automated Creation Example:**\n```bash\nuvx agent-starter-pack create my-automated-agent \\\n  -a adk_base \\\n  -d cloud_run \\\n  --region us-central1 \\\n  --auto-approve\n```\n\n---\n\n### `agent-starter-pack enhance` Command\n\nEnhance your existing project with AI agent capabilities by adding agent-starter-pack features in-place. This command supports all the same options as `create` but templates directly into the current directory instead of creating a new project directory.\n\n**Usage:**\n```bash\nagent-starter-pack enhance [TEMPLATE_PATH] [OPTIONS]\n```\n\n**Key Differences from `create`:**\n*   Templates into current directory (equivalent to `create --in-folder`)\n*   `TEMPLATE_PATH` defaults to current directory (`.`)\n*   Project name defaults to current directory name\n*   Additional `--base-template` option to override template inheritance\n\n**Enhanced Project Example:**\n```bash\n# Enhance current directory with agent capabilities\nuvx agent-starter-pack enhance . \\\n  --base-template adk_base \\\n  -d cloud_run \\\n  --region us-central1 \\\n  --auto-approve\n```\n\n**Project Structure:** Expects agent code in `app/` directory (configurable via `--agent-directory`).\n\n---\n\n### Available Agent Templates\n\nTemplates for the `create` command (via `-a` or `--agent`):\n\n| Agent Name             | Description                                  |\n| :--------------------- | :------------------------------------------- |\n| `adk_base`             | Base ReAct agent (ADK)                       |\n| `adk_gemini_fullstack` | Production-ready fullstack research agent    |\n| `agentic_rag`          | RAG agent for document retrieval & Q&A       |\n| `langgraph_base_react` | Base ReAct agent (LangGraph)                 |\n| `crewai_coding_crew`   | Multi-agent collaborative coding assistance  |\n| `live_api`             | Real-time multimodal RAG agent               |\n\n---\n\n### Including a Data Ingestion Pipeline (for RAG agents)\n\nFor RAG agents needing custom document search, enabling this option automates loading, chunking, embedding documents with Vertex AI, and storing them in a vector database.\n\n**How to enable:**\n```bash\nuvx agent-starter-pack create my-rag-agent \\\n  -a agentic_rag \\\n  -d cloud_run \\\n  -i \\\n  -ds vertex_ai_search \\\n  --auto-approve\n```\n**Post-creation:** Follow your new project's `data_ingestion/README.md` to deploy the necessary infrastructure.\n\n---\n### Section 3: Development & Automated Deployment Workflow\n---\n\nThis section describes the end-to-end lifecycle of an agent, with emphasis on automation.\n\n\n### 1. Local Development & Iteration\n\nOnce your project is created, navigate into its directory to begin development.\n\n**First, install dependencies (run once):**\n```bash\nmake install\n```\n\n**Next, test your agent. The recommended method is to use a programmatic script.**\n\n#### Programmatic Testing (Recommended Workflow)\n\nThis method allows for quick, automated validation of your agent's logic.\n\n1.  **Create a script:** In the project's root directory, create a Python script named `run_agent.py`.\n2.  **Invoke the agent:** In the script, write code to programmatically call your agent with sample input and `print()` the output for inspection.\n    *   **Guidance:** If you're unsure or no guidance exists, you can look at files in the `tests/` directory for examples of how to import and call the agent's main function.\n    *   **Important:** This script is for simple validation. **Assertions are not required**, and you should not create a formal `pytest` file.\n3.  **Run the test:** Execute your script from the terminal using `uv`.\n    ```bash\n    uv run python run_agent.py\n    ```\nYou can keep the test file for future testing.\n\n#### Manual Testing with the UI Playground (Optional)\n\nIf the user needs to interact with your agent manually in a chat interface for debugging:\n\n1.  Run the following command to start the local web UI:\n    ```bash\n    make playground\n    ```\n    This is useful for human-in-the-loop testing and features hot-reloading.\n\n### 2. Deploying to a Cloud Development Environment\nBefore setting up full CI/CD, you can deploy to a personal cloud dev environment.\n\n1.  **Set Project:** `gcloud config set project YOUR_DEV_PROJECT_ID`\n2.  **Provision Resources:** `make setup-dev-env` (uses Terraform).\n3.  **Deploy Backend:** `make backend` (builds and deploys the agent).\n\n### 3. Automated Production-Ready Deployment with CI/CD\nFor reliable deployments, the `setup-cicd` command streamlines the entire process. It creates a GitHub repo, connects it to your chosen CI/CD runner (Google Cloud Build or GitHub Actions), provisions staging/prod infrastructure, and configures deployment triggers.\n\n**Automated CI/CD Setup Example (Recommended):**\n```bash\n# Run from the project root. This command will guide you or can be automated with flags.\nuvx agent-starter-pack setup-cicd\n```\n\n**CI/CD Workflow Logic:**\n*   **On Pull Request:** CI pipeline runs tests.\n*   **On Merge to `main`:** CD pipeline deploys to staging.\n*   **Manual Approval:** A manual approval step triggers the production deployment.\n\n---\n### Section 4: Key Features & Customization\n---\n\n### Deploying with a User Interface (UI)\n*   **Unified Deployment (for Dev/Test):** The backend and frontend can be packaged and served from a single Cloud Run service, secured with Identity-Aware Proxy (IAP).\n*   **Deploying with UI:** `make backend IAP=true`\n*   **Access Control:** After deploying with IAP, grant users the `IAP-secured Web App User` role in IAM to give them access.\n\n### Session Management\n\nFor stateful agents, the starter pack supports persistent sessions.\n*   **Cloud Run:** Choose between `in_memory` (for testing) and durable `alloydb` sessions using the `--session-type` flag.\n*   **Agent Engine:** Provides session management automatically.\n\n### Monitoring & Observability\n*   **Technology:** Uses OpenTelemetry to emit events to Google Cloud Trace and Logging.\n*   **Custom Tracer:** A custom tracer in `app/utils/tracing.py` (or a different agent directory instead of app) handles large payloads by linking to GCS, overcoming default service limits.\n*   **Infrastructure:** A Log Router to sink data to BigQuery is provisioned by Terraform.\n\n---\n### Section 5: CLI Reference for CI/CD Setup\n---\n\n### `agent-starter-pack setup-cicd`\nAutomates the complete CI/CD infrastructure setup for GitHub-based deployments. Intelligently detects your CI/CD runner (Google Cloud Build or GitHub Actions) and configures everything automatically.\n\n**Usage:**\n```bash\nuvx agent-starter-pack setup-cicd [OPTIONS]\n```\n\n**Prerequisites:** \n- Run from the project root (directory with `pyproject.toml`)\n- Required tools: `gh` CLI (authenticated), `gcloud` CLI (authenticated), `terraform`\n- `Owner` role on GCP projects\n- GitHub token with `repo` and `workflow` scopes\n\n**Key Options:**\n*   `--staging-project`, `--prod-project`: GCP project IDs (will prompt if omitted).\n*   `--repository-name`, `--repository-owner`: GitHub repo details (will prompt if omitted).\n*   `--cicd-project`: CI/CD resources project (defaults to prod project).\n*   `--dev-project`: Development project ID (optional).\n*   `--region`: GCP region (default: `us-central1`).\n*   `--auto-approve`: Skip all interactive prompts.\n*   `--local-state`: Use local Terraform state instead of GCS backend.\n*   `--debug`: Enable debug logging.\n\n**What it does:**\n1. Creates/connects GitHub repository\n2. Sets up Terraform infrastructure with remote state\n3. Configures CI/CD runner connection (Cloud Build or GitHub Actions with WIF)\n4. Provisions staging/prod environments\n5. Sets up local Git repository with origin remote\n\n**Automated Example:**\n```bash\nuvx agent-starter-pack setup-cicd \\\n  --staging-project your-staging-project \\\n  --prod-project your-prod-project \\\n  --repository-name your-repo-name \\\n  --repository-owner your-username \\\n  --auto-approve\n```\n\n**After setup, push to trigger pipeline:**\n```bash\ngit add . && git commit -m \"Initial commit\" && git push -u origin main\n```\n\n* Note: For coding agents - ask user for required project IDs and repo details before running with `--auto-approve`.\n* Note: If user prefers different git provider, refer to `deployment/README.md` for manual deployment.\n---\n### Section 6: Operational Guidelines for Coding Agents\n\nThese guidelines are essential for interacting with the Agent Starter Pack project effectively.\n\n---\n\n### Principle 1: Code Preservation & Isolation\n\nWhen executing code modifications using tools like `replace` or `write_file`, your paramount objective is surgical precision. You **must alter only the code segments directly targeted** by the user's request, while **strictly preserving all surrounding and unrelated code.**\n\n**Mandatory Pre-Execution Verification:**\n\nBefore finalizing any `new_string` for a `replace` operation, meticulously verify the following:\n\n1.  **Target Identification:** Clearly define the exact lines or expressions to be changed, based *solely* on the user's explicit instructions.\n2.  **Preservation Check:** Compare your proposed `new_string` against the `old_string`. Ensure all code, configuration values (e.g., `model`, `version`, `api_key`), comments, and formatting *outside* the identified target remain identical and verbatim.\n\n**Example: Adhering to Preservation**\n\n*   **User Request:** \"Change the agent's instruction to be a recipe suggester.\"\n*   **Original Code Snippet:**\n    ```python\n    root_agent = Agent(\n        name=\"root_agent\",\n        model=\"gemini-2.5-flash\",\n        instruction=\"You are a helpful AI assistant.\"\n    )\n    ```\n*   **Incorrect Modification (VIOLATION):**\n    ```python\n    root_agent = Agent(\n        name=\"recipe_suggester\",\n        model=\"gemini-1.5-flash\", # UNINTENDED MUTATION - model was not requested to change\n        instruction=\"You are a recipe suggester.\"\n    )\n    ```\n*   **Correct Modification (COMPLIANT):**\n    ```python\n    root_agent = Agent(\n        name=\"recipe_suggester\", # OK, related to new purpose\n        model=\"gemini-2.5-flash\", # MUST be preserved\n        instruction=\"You are a recipe suggester.\" # OK, the direct target\n    )\n    ```\n\n**Critical Error:** Failure to adhere to this preservation principle is a critical error. Always prioritize the integrity of existing, unchanged code over the convenience of rewriting entire blocks.\n\n---\n\n### Principle 2: Workflow & Execution Best Practices\n\n*   **Standard Workflow:**\n    The validated end-to-end process is: `create` \u2192 `test` \u2192 `setup-cicd` \u2192 push to deploy. Trust this high-level workflow as the default for developing and shipping agents.\n\n*   **Agent Testing:**\n    *   **Avoid `make playground`** unless specifically instructed; it is designed for human interaction. Focus on programmatic testing.\n\n*   **Model Selection:**\n    *   **When using Gemini, prefer the 2.5 model family** for optimal performance and capabilities: \"gemini-2.5-pro\" and \"gemini-2.5-flash\"\n\n*   **Running Python Commands:**\n    *   Always use `uv` to execute Python commands within this repository (e.g., `uv run run_agent.py`).\n    *   Ensure project dependencies are installed by running `make install` before executing scripts.\n    *   Consult the project's `Makefile` and `README.md` for other useful development commands.\n\n*   **Further Reading & Troubleshooting:**\n    *   For questions about specific frameworks (e.g., LangGraph) or Google Cloud products (e.g., Cloud Run), their official documentation and online resources are the best source of truth.\n    *   **When encountering persistent errors or if you're unsure how to proceed after initial troubleshooting, a targeted Google Search is strongly recommended.** It is often the fastest way to find relevant documentation, community discussions, or direct solutions to your problem.\n"
  }
}